
PMID- 26529458
OWN - NLM
STAT- In-Data-Review
DA  - 20151104
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
VI  - 21
IP  - 12
DP  - 2015 Dec
TI  - Interactive Near-Field Illumination for Photorealistic Augmented Reality with
      Varying Materials on Mobile Devices.
PG  - 1349-62
LID - 10.1109/TVCG.2015.2450717 [doi]
AB  - At present, photorealistic augmentation is not yet possible since the
      computational power of mobile devices is insufficient. Even streaming solutions
      from stationary PCs cause a latency that affects user interactions considerably. 
      Therefore, we introduce a differential rendering method that allows for a
      consistent illumination of the inserted virtual objects on mobile devices,
      avoiding delays. The computation effort is shared between a stationary PC and the
      mobile devices to make use of the capacities available on both sides. The method 
      is designed such that only a minimum amount of data has to be transferred
      asynchronously between the participants. This allows for an interactive
      illumination of virtual objects with a consistent appearance under both
      temporally and spatially varying real illumination conditions. To describe the
      complex near-field illumination in an indoor scenario, HDR video cameras are used
      to capture the illumination from multiple directions. In this way, sources of
      illumination can be considered that are not directly visible to the mobile device
      because of occlusions and the limited field of view. While our method focuses on 
      Lambertian materials, we also provide some initial approaches to approximate
      non-diffuse virtual objects and thereby allow for a wider field of application at
      nearly the same cost.
FAU - Rohmer, Kai
AU  - Rohmer K
FAU - Buschel, Wolfgang
AU  - Buschel W
FAU - Dachselt, Raimund
AU  - Dachselt R
FAU - Grosch, Thorsten
AU  - Grosch T
LA  - eng
PT  - Journal Article
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
EDAT- 2015/11/04 06:00
MHDA- 2015/11/04 06:00
CRDT- 2015/11/04 06:00
AID - 10.1109/TVCG.2015.2450717 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2015 Dec;21(12):1349-62. doi:
      10.1109/TVCG.2015.2450717.

PMID- 26474605
OWN - NLM
STAT- Publisher
DA  - 20151017
LR  - 20151018
IS  - 1553-3514 (Electronic)
IS  - 1553-3506 (Linking)
DP  - 2015 Oct 15
TI  - Indocyanine Green Fluorescence for Free-Flap Perfusion Imaging Revisited:
      Advanced Decision Making by Virtual Perfusion Reality in Visionsense Fusion
      Imaging Angiography.
LID - 1553350615610651 [pii]
AB  - BACKGROUND: Near-infrared indocyanine green video angiography (ICG-NIR-VA) has
      been introduced for free-flap surgery and may provide intraoperative flap
      designing as well as postoperative monitoring. Nevertheless, the technique has
      not been established in clinical routine because of controversy over benefits.
      Improved technical features of the novel Visionsense ICG-NIR-VA surgery system
      are promising to revisit the field of application. It features a unique real-time
      fusion image of simultaneous NIR and white light visualization, with highlighted 
      perfusion, including a color-coded perfusion flow scale for optimized anatomical 
      understanding. METHODS: In a feasibility study, the Visionsense ICG-NIR-VA system
      was applied during 10 free-flap surgeries in 8 patients at our center.
      Indications included anterior lateral thigh (ALT) flap (n = 4), latissimus dorsi 
      muscle flap (n = 1), tensor fascia latae flap (n = 1), and two bilateral deep
      inferior epigastric artery perforator flaps (n = 4). The system was used
      intraoperatively and postoperatively to investigate its impact on surgical
      decision making and to observe perfusion patterns correlated to clinical
      monitoring. RESULTS: Visionsense ICG-NIR-VA aided assessing free-flap design and 
      perfusion patterns in all cases and correlated with clinical observations.
      Additional interventions were performed in 2 cases (22%). One venous anastomosis 
      was revised, and 1 flap was redesigned. Indicated by ICG-NIR-VA, 1 ALT flap
      developed partial flap necrosis (11%). CONCLUSIONS: The Visionsense ICG-NIR-VA
      system allowed a virtual view of flap perfusion anatomy by fusion imaging in
      real-time. The system improved decision making for flap design and surgical
      decisions. Clinical and ICG-NIR-VA parameters correlated. Its future
      implementation may aid in improving outcomes for free-flap surgery, but
      additional experience is needed to define its final role.
CI  - (c) The Author(s) 2015.
FAU - Bigdeli, Amir Khosrow
AU  - Bigdeli AK
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Gazyakan, Emre
AU  - Gazyakan E
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Schmidt, Volker Juergen
AU  - Schmidt VJ
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Hernekamp, Frederick Jochen
AU  - Hernekamp FJ
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Harhaus, Leila
AU  - Harhaus L
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Henzler, Thomas
AU  - Henzler T
AD  - Medical Faculty Mannheim, University of Heidelberg, Mannheim, Germany.
FAU - Kremer, Thomas
AU  - Kremer T
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Kneser, Ulrich
AU  - Kneser U
AD  - University of Heidelberg, Heidelberg, Germany.
FAU - Hirche, Christoph
AU  - Hirche C
AD  - University of Heidelberg, Heidelberg, Germany
      Christoph.Hirche@bgu-ludwigshafen.de.
LA  - ENG
PT  - JOURNAL ARTICLE
DEP - 20151015
TA  - Surg Innov
JT  - Surgical innovation
JID - 101233809
OTO - NOTNLM
OT  - 3D anatomy
OT  - ICG
OT  - Visionsense
OT  - angiography
OT  - free flap
OT  - fusion image
OT  - indocyanine green
OT  - microsurgery
EDAT- 2015/10/18 06:00
MHDA- 2015/10/18 06:00
CRDT- 2015/10/18 06:00
AID - 1553350615610651 [pii]
AID - 10.1177/1553350615610651 [doi]
PST - aheadofprint
SO  - Surg Innov. 2015 Oct 15. pii: 1553350615610651.

PMID- 26357242
OWN - NLM
STAT- In-Data-Review
DA  - 20150911
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
VI  - 21
IP  - 7
DP  - 2015 Jul
TI  - Effects of Field of View and Visual Complexity on Virtual Reality Training
      Effectiveness for a Visual Scanning Task.
PG  - 794-807
LID - 10.1109/TVCG.2015.2403312 [doi]
AB  - Virtual reality training systems are commonly used in a variety of domains, and
      it is important to understand how the realism of a training simulation influences
      training effectiveness. We conducted a controlled experiment to test the effects 
      of display and scenario properties on training effectiveness for a visual
      scanning task in a simulated urban environment. The experiment varied the levels 
      of field of view and visual complexity during a training phase and then evaluated
      scanning performance with the simulator's highest levels of fidelity and scene
      complexity. To assess scanning performance, we measured target detection and
      adherence to a prescribed strategy. The results show that both field of view and 
      visual complexity significantly affected target detection during training; higher
      field of view led to better performance and higher visual complexity worsened
      performance. Additionally, adherence to the prescribed visual scanning strategy
      during assessment was best when the level of visual complexity during training
      matched that of the assessment conditions, providing evidence that similar visual
      complexity was important for learning the technique. The results also demonstrate
      that task performance during training was not always a sufficient measure of
      mastery of an instructed technique. That is, if learning a prescribed strategy or
      skill is the goal of a training exercise, performance in a simulation may not be 
      an appropriate indicator of effectiveness outside of training-evaluation in a
      more realistic setting may be necessary.
FAU - Ragan, Eric D
AU  - Ragan ED
FAU - Bowman, Doug A
AU  - Bowman DA
FAU - Kopper, Regis
AU  - Kopper R
FAU - Stinson, Cheryl
AU  - Stinson C
FAU - Scerbo, Siroberto
AU  - Scerbo S
FAU - McMahan, Ryan P
AU  - McMahan RP
LA  - eng
PT  - Journal Article
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
EDAT- 2015/09/12 06:00
MHDA- 2015/09/12 06:00
CRDT- 2015/09/11 06:00
AID - 10.1109/TVCG.2015.2403312 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2015 Jul;21(7):794-807. doi:
      10.1109/TVCG.2015.2403312.

PMID- 26326766
OWN - NLM
STAT- In-Data-Review
DA  - 20150902
IS  - 1534-7362 (Electronic)
IS  - 1534-7362 (Linking)
VI  - 15
IP  - 12
DP  - 2015 Sep 1
TI  - Depth Perception of Augmented Reality Information in an Automotive Contact Analog
      Head-Up Display.
PG  - 1078
LID - 10.1167/15.12.1078 [doi]
AB  - In an automotive contact analog head-up display (cHUD), virtual information can
      be presented in augmented reality manner in the driver's primary field of view,
      minimizing mental transformation effort for the driver. Whether virtual
      information in the cHUD is really perceived at the correct location in the
      environment depends on several factors such as the technological specifications
      of the cHUD, the design of the virtual information or environmental conditions.
      One possibility to realize a cHUD is to implement an upright virtual image in a
      distance at which human depth perception is supposed to rely on monocular depth
      cues only (&gt; 6m). So far it has not been investigated whether this technology 
      succeeds at conveying the correct spatial impression of the virtual information
      to the driver. In the current study, we investigated whether depth perception of 
      lying navigation arrows in a cHUD with a fixed virtual image distance of 10m was 
      influenced by (a) whether the arrow was overlaid on the road surface or
      superimposed on a leading vehicle and by (b) whether it was shaded/ desaturated
      with increasing distance or not. Arrows were randomly presented for 1sec at
      suggested distances of 20, 25, 30, 35, and 40m. Participants had to match the
      virtual arrows in the cHUD to distance markers in front of the car.
      Superimposition led to a significant increase in errors compared to the
      projection onto the street surface but did not affect reaction times. Increasing 
      distance led to significantly increased reaction times but fewer errors
      (speed-accuracy tradeoff). The impact of superimposition on error rates was
      significantly more pronounced in nearer distances. Shading did not have an effect
      on error rates or reaction times. The results have important implications for the
      further development of automotive cHUD technology and related design concepts.
      Meeting abstract presented at VSS 2015.
FAU - Pfannmuller, Lisa
AU  - Pfannmuller L
FAU - Walter, Matthias
AU  - Walter M
FAU - Senner, Bernhard
AU  - Senner B
FAU - Bengler, Klaus
AU  - Bengler K
LA  - eng
PT  - Journal Article
PL  - United States
TA  - J Vis
JT  - Journal of vision
JID - 101147197
SB  - IM
EDAT- 2015/09/02 06:00
MHDA- 2015/09/02 06:00
CRDT- 2015/09/02 06:00
AID - 2434188 [pii]
AID - 10.1167/15.12.1078 [doi]
PST - ppublish
SO  - J Vis. 2015 Sep 1;15(12):1078. doi: 10.1167/15.12.1078.

PMID- 26097447
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20150622
DCOM- 20150622
LR  - 20150624
IS  - 1662-4548 (Print)
IS  - 1662-453X (Linking)
VI  - 9
DP  - 2015
TI  - Rapid P300 brain-computer interface communication with a head-mounted display.
PG  - 207
LID - 10.3389/fnins.2015.00207 [doi]
AB  - Visual ERP (P300) based brain-computer interfaces (BCIs) allow for fast and
      reliable spelling and are intended as a muscle-independent communication channel 
      for people with severe paralysis. However, they require the presentation of
      visual stimuli in the field of view of the user. A head-mounted display could
      allow convenient presentation of visual stimuli in situations, where mounting a
      conventional monitor might be difficult or not feasible (e.g., at a patient's
      bedside). To explore if similar accuracies can be achieved with a virtual reality
      (VR) headset compared to a conventional flat screen monitor, we conducted an
      experiment with 18 healthy participants. We also evaluated it with a person in
      the locked-in state (LIS) to verify that usage of the headset is possible for a
      severely paralyzed person. Healthy participants performed online spelling with
      three different display methods. In one condition a 5 x 5 letter matrix was
      presented on a conventional 22 inch TFT monitor. Two configurations of the VR
      headset were tested. In the first (glasses A), the same 5 x 5 matrix filled the
      field of view of the user. In the second (glasses B), single letters of the
      matrix filled the field of view of the user. The participant in the LIS tested
      the VR headset on three different occasions (glasses A condition only). For
      healthy participants, average online spelling accuracies were 94% (15.5 bits/min)
      using three flash sequences for spelling with the monitor and glasses A and 96%
      (16.2 bits/min) with glasses B. In one session, the participant in the LIS
      reached an online spelling accuracy of 100% (10 bits/min) using the glasses A
      condition. We also demonstrated that spelling with one flash sequence is possible
      with the VR headset for healthy users (mean: 32.1 bits/min, maximum reached by
      one user: 71.89 bits/min at 100% accuracy). We conclude that the VR headset
      allows for rapid P300 BCI communication in healthy users and may be a suitable
      display option for severely paralyzed persons.
FAU - Kathner, Ivo
AU  - Kathner I
AD  - Institute of Psychology, University of Wurzburg Wurzburg, Germany.
FAU - Kubler, Andrea
AU  - Kubler A
AD  - Institute of Psychology, University of Wurzburg Wurzburg, Germany.
FAU - Halder, Sebastian
AU  - Halder S
AD  - Institute of Psychology, University of Wurzburg Wurzburg, Germany ; Department of
      Rehabilitation for Brain Functions, Research Institute of National Rehabilitation
      Center for Persons with Disabilities Tokorozawa, Japan.
LA  - eng
PT  - Journal Article
DEP - 20150605
PL  - Switzerland
TA  - Front Neurosci
JT  - Frontiers in neuroscience
JID - 101478481
PMC - PMC4456572
OID - NLM: PMC4456572
OTO - NOTNLM
OT  - P300
OT  - brain-computer interface
OT  - head-mounted display
OT  - locked-in state
OT  - rapid BCI
EDAT- 2015/06/23 06:00
MHDA- 2015/06/23 06:01
CRDT- 2015/06/23 06:00
PHST- 2015 [ecollection]
PHST- 2015/02/06 [received]
PHST- 2015/05/23 [accepted]
PHST- 2015/06/05 [epublish]
AID - 10.3389/fnins.2015.00207 [doi]
PST - epublish
SO  - Front Neurosci. 2015 Jun 5;9:207. doi: 10.3389/fnins.2015.00207. eCollection
      2015.

PMID- 26082735
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20150617
DCOM- 20150617
LR  - 20150619
IS  - 1664-1078 (Electronic)
IS  - 1664-1078 (Linking)
VI  - 6
DP  - 2015
TI  - Comparing the effectiveness of different displays in enhancing illusions of
      self-movement (vection).
PG  - 713
LID - 10.3389/fpsyg.2015.00713 [doi]
AB  - Illusions of self-movement (vection) can be used in virtual reality (VR) and
      other applications to give users the embodied sensation that they are moving when
      physical movement is unfeasible or too costly. Whereas a large body of vection
      literature studied how various parameters of the presented visual stimulus affect
      vection, little is known how different display types might affect vection. As a
      step toward addressing this gap, we conducted three experiments to compare
      vection and usability parameters between commonly used VR displays, ranging from 
      stereoscopic projection and 3D TV to high-end head-mounted display (HMD, NVIS
      SX111) and recent low-cost HMD (Oculus Rift). The last experiment also compared
      these two HMDs in their native full field of view (FOV) and a reduced, matched
      FOV of 72 degrees x 45 degrees . Participants moved along linear and curvilinear 
      paths in the virtual environment, reported vection onset time, and rated vection 
      intensity at the end of each trial. In addition, user ratings on immersion,
      motion sickness, vection, and overall preference were recorded retrospectively
      and compared between displays. Unexpectedly, there were no significant effects of
      display on vection measures. Reducing the FOV for the HMDs (from full to 72
      degrees x 45 degrees ) decreased vection onset latencies, but did not affect
      vection intensity. As predicted, curvilinear paths yielded earlier and more
      intense vection. Although vection has often been proposed to predict or even
      cause motion sickness, we observed no correlation for any of the displays
      studied. In conclusion, perceived self-motion and other user experience measures 
      proved surprisingly tolerant toward changes in display type as long as the FOV
      was roughly matched. This suggests that display choice for vection research and
      VR applications can be largely based on other considerations as long as the
      provided FOV is sufficiently large.
FAU - Riecke, Bernhard E
AU  - Riecke BE
AD  - iSpace Lab, School of Interactive Arts and Technology, Simon Fraser University , 
      Surrey, BC, Canada.
FAU - Jordan, Jacqueline D
AU  - Jordan JD
AD  - iSpace Lab, School of Interactive Arts and Technology, Simon Fraser University , 
      Surrey, BC, Canada.
LA  - eng
PT  - Journal Article
DEP - 20150601
PL  - Switzerland
TA  - Front Psychol
JT  - Frontiers in psychology
JID - 101550902
PMC - PMC4450174
OID - NLM: PMC4450174
OTO - NOTNLM
OT  - display technologies
OT  - head-mounted displays
OT  - optic flow
OT  - self-motion illusion
OT  - self-motion simulation
OT  - vection
OT  - virtual reality
OT  - visually induced motion sickness
EDAT- 2015/06/18 06:00
MHDA- 2015/06/18 06:01
CRDT- 2015/06/18 06:00
PHST- 2015 [ecollection]
PHST- 2014/12/11 [received]
PHST- 2015/05/13 [accepted]
PHST- 2015/06/01 [epublish]
AID - 10.3389/fpsyg.2015.00713 [doi]
PST - epublish
SO  - Front Psychol. 2015 Jun 1;6:713. doi: 10.3389/fpsyg.2015.00713. eCollection 2015.

PMID- 26024818
OWN - NLM
STAT- In-Process
DA  - 20150817
IS  - 1361-8423 (Electronic)
IS  - 1361-8415 (Linking)
VI  - 25
IP  - 1
DP  - 2015 Oct
TI  - Pico Lantern: Surface reconstruction and augmented reality in laparoscopic
      surgery using a pick-up laser projector.
PG  - 95-102
LID - 10.1016/j.media.2015.04.008 [doi]
LID - S1361-8415(15)00058-4 [pii]
AB  - The Pico Lantern is a miniature projector developed for structured light surface 
      reconstruction, augmented reality and guidance in laparoscopic surgery. During
      surgery it will be dropped into the patient and picked up by a laparoscopic tool.
      While inside the patient it projects a known coded pattern and images onto the
      surface of the tissue. The Pico Lantern is visually tracked in the laparoscope's 
      field of view for the purpose of stereo triangulation between it and the
      laparoscope. In this paper, the first application is surface reconstruction.
      Using a stereo laparoscope and an untracked Pico Lantern, the absolute error for 
      surface reconstruction for a plane, cylinder and ex vivo kidney, is 2.0 mm, 3.0
      mm and 5.6 mm, respectively. Using a mono laparoscope and a tracked Pico Lantern 
      for the same plane, cylinder and kidney the absolute error is 1.4 mm, 1.5 mm and 
      1.5 mm, respectively. These results confirm the benefit of the wider baseline
      produced by tracking the Pico Lantern. Virtual viewpoint images are generated
      from the kidney surface data and an in vivo proof-of-concept porcine trial is
      reported. Surface reconstruction of the neck of a volunteer shows that the
      pulsatile motion of the tissue overlying a major blood vessel can be detected and
      displayed in vivo. Future work will integrate the Pico Lantern into standard and 
      robot-assisted laparoscopic surgery.
CI  - Copyright (c) 2015 Elsevier B.V. All rights reserved.
FAU - Edgcumbe, Philip
AU  - Edgcumbe P
AD  - MD/PhD Program, Biomedical Engineering Program, University of British Columbia,
      Vancouver, BC, Canada. Electronic address: edgcumbe@ece.ubc.ca.
FAU - Pratt, Philip
AU  - Pratt P
AD  - Hamlyn Centre for Robotic Surgery, Imperial College of Science, Technology and
      Medicine, London, UK.
FAU - Yang, Guang-Zhong
AU  - Yang GZ
AD  - Hamlyn Centre for Robotic Surgery, Imperial College of Science, Technology and
      Medicine, London, UK.
FAU - Nguan, Christopher
AU  - Nguan C
AD  - Department of Urologic Sciences, University of British Columbia, Vancouver, BC,
      Canada.
FAU - Rohling, Robert
AU  - Rohling R
AD  - Department of Electrical Engineering and Computer Engineering, University of
      British Columbia, Vancouver, BC, Canada; Department of Mechanical Engineering,
      University of British Columbia, Vancouver, BC, Canada.
LA  - eng
GR  - Canadian Institutes of Health Research/Canada
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20150507
PL  - Netherlands
TA  - Med Image Anal
JT  - Medical image analysis
JID - 9713490
SB  - IM
OTO - NOTNLM
OT  - Augmented reality
OT  - Laparoscopic surgery
OT  - Pico Lantern
OT  - Pico projector
EDAT- 2015/05/31 06:00
MHDA- 2015/05/31 06:00
CRDT- 2015/05/31 06:00
PHST- 2014/11/17 [received]
PHST- 2015/04/08 [revised]
PHST- 2015/04/09 [accepted]
PHST- 2015/05/07 [aheadofprint]
AID - S1361-8415(15)00058-4 [pii]
AID - 10.1016/j.media.2015.04.008 [doi]
PST - ppublish
SO  - Med Image Anal. 2015 Oct;25(1):95-102. doi: 10.1016/j.media.2015.04.008. Epub
      2015 May 7.

PMID- 25992790
OWN - NLM
STAT- In-Process
DA  - 20150521
LR  - 20150528
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 10
IP  - 5
DP  - 2015
TI  - Virtual Viewing Time: The Relationship between Presence and Sexual Interest in
      Androphilic and Gynephilic Men.
PG  - e0127156
LID - 10.1371/journal.pone.0127156 [doi]
AB  - Virtual Reality (VR) has successfully been used in the research of human behavior
      for more than twenty years. The main advantage of VR is its capability to induce 
      a high sense of presence. This results in emotions and behavior which are very
      close to those shown in real situations. In the context of sex research, only a
      few studies have used high-immersive VR so far. The ones that did can be found
      mostly in the field of forensic psychology. Nevertheless, the relationship
      between presence and sexual interest still remains unclear. The present study is 
      the first to examine the advantages of high-immersive VR in comparison to a
      conventional standard desktop system regarding their capability to measure sexual
      interest. 25 gynephilic and 20 androphilic healthy men underwent three
      experimental conditions, which differed in their ability to induce a sense of
      presence. In each condition, participants were asked to rate ten male and ten
      female virtual human characters regarding their sexual attractiveness. Without
      their knowledge, the subjects' viewing time was assessed throughout the rating.
      Subjects were then asked to rate the sense of presence they had experienced as
      well as their perceived realism of the characters. Results suggested that
      stereoscopic viewing can significantly enhance the subjective sexual
      attractiveness of sexually relevant characters. Furthermore, in all three
      conditions participants looked significantly longer at sexually relevant virtual 
      characters than at sexually non-relevant ones. The high immersion condition
      provided the best discriminant validity. From a statistical point of view,
      however, the sense of presence had no significant influence on the discriminant
      validity of the viewing time task. The study showed that high-immersive virtual
      environments enhance realism ratings as well as ratings of sexual attractiveness 
      of three-dimensional human stimuli in comparison to standard desktop systems.
      Results also show that viewing time seems to be influenced neither by sexual
      attractiveness nor by realism of stimuli. This indicates how important task
      specific mechanisms of the viewing time effect are.
FAU - Fromberger, Peter
AU  - Fromberger P
AD  - Ludwig-Meyer-Institute for Forensic Psychiatry and Psychotherapy, Faculty of
      Medicine, Georg-August-University, Gottingen, Lower Saxony, Germany.
FAU - Meyer, Sabrina
AU  - Meyer S
AD  - Ludwig-Meyer-Institute for Forensic Psychiatry and Psychotherapy, Faculty of
      Medicine, Georg-August-University, Gottingen, Lower Saxony, Germany.
FAU - Kempf, Christina
AU  - Kempf C
AD  - Ludwig-Meyer-Institute for Forensic Psychiatry and Psychotherapy, Faculty of
      Medicine, Georg-August-University, Gottingen, Lower Saxony, Germany.
FAU - Jordan, Kirsten
AU  - Jordan K
AD  - Ludwig-Meyer-Institute for Forensic Psychiatry and Psychotherapy, Faculty of
      Medicine, Georg-August-University, Gottingen, Lower Saxony, Germany.
FAU - Muller, Jurgen L
AU  - Muller JL
AD  - Ludwig-Meyer-Institute for Forensic Psychiatry and Psychotherapy, Faculty of
      Medicine, Georg-August-University, Gottingen, Lower Saxony, Germany.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20150518
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
PMC - PMC4436365
OID - NLM: PMC4436365
EDAT- 2015/05/21 06:00
MHDA- 2015/05/21 06:00
CRDT- 2015/05/21 06:00
PHST- 2015 [ecollection]
PHST- 2014/10/14 [received]
PHST- 2015/04/13 [accepted]
PHST- 2015/05/18 [epublish]
AID - 10.1371/journal.pone.0127156 [doi]
AID - PONE-D-14-46066 [pii]
PST - epublish
SO  - PLoS One. 2015 May 18;10(5):e0127156. doi: 10.1371/journal.pone.0127156.
      eCollection 2015.

PMID- 25967333
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20150513
DCOM- 20150715
IS  - 1539-4522 (Electronic)
IS  - 0003-6935 (Linking)
VI  - 54
IP  - 11
DP  - 2015 Apr 10
TI  - Slim near-eye display using pinhole aperture arrays.
PG  - 3422-7
LID - 10.1364/AO.54.003422 [doi]
AB  - We report a new technique for building a wide-angle, lightweight,
      thin-form-factor, cost-effective, easy-to-manufacture near-eye head-mounted
      display (HMD) for virtual reality applications. Our approach adopts an aperture
      mask containing an array of pinholes and a screen as a source of imagery. We
      demonstrate proof-of-concept HMD prototypes with a binocular field of view (FOV) 
      of 70 degrees x45 degrees , or total diagonal FOV of 83 degrees . This FOV should
      increase with increasing display panel size. The optical angular resolution
      supported in our prototype can go down to 1.4-2.1 arcmin by adopting a display
      with 20-30 mum pixel pitch.
FAU - Aksit, Kaan
AU  - Aksit K
FAU - Kautz, Jan
AU  - Kautz J
FAU - Luebke, David
AU  - Luebke D
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Appl Opt
JT  - Applied optics
JID - 0247660
EDAT- 2015/05/15 06:00
MHDA- 2015/05/15 06:01
CRDT- 2015/05/14 06:00
AID - 315125 [pii]
PST - ppublish
SO  - Appl Opt. 2015 Apr 10;54(11):3422-7. doi: 10.1364/AO.54.003422.

PMID- 25946099
OWN - NLM
STAT- MEDLINE
DA  - 20150507
DCOM- 20150714
IS  - 1538-9235 (Electronic)
IS  - 1040-5488 (Linking)
VI  - 92
IP  - 4
DP  - 2015 Apr
TI  - Does correcting astigmatism with toric lenses improve driving performance?
PG  - 404-11
LID - 10.1097/OPX.0000000000000554 [doi]
AB  - PURPOSE: Driving is a vision-based activity of daily living that impacts safety. 
      Because visual disruption can compromise driving safety, contact lens wearers
      with astigmatism may pose a driving safety risk if they experience residual blur 
      from spherical lenses that do not correct their astigmatism or if they experience
      blur from toric lenses that rotate excessively. Given that toric lens
      stabilization systems are continually improving, this preliminary study tested
      the hypothesis that astigmats wearing toric contact lenses, compared with
      spherical lenses, would exhibit better overall driving performance and
      driving-specific visual abilities. METHODS: A within-subject, single-blind,
      crossover, randomized design was used to evaluate driving performance in 11 young
      adults with astigmatism (-0.75 to -1.75 diopters cylinder). Each participant
      drove a highly immersive, virtual reality driving simulator (210 degrees field of
      view) with (1) no correction, (2) spherical contact lens correction (ACUVUE
      MOIST), and (3) toric contact lens correction (ACUVUE MOIST for Astigmatism).
      Tactical driving skills such as steering, speed management, and braking, as well 
      as operational driving abilities such as visual acuity, contrast sensitivity, and
      foot and arm reaction time, were quantified. RESULTS: There was a main effect for
      type of correction on driving performance (p = 0.05). Correction with toric
      lenses resulted in significantly safer tactical driving performance than no
      correction (p < 0.05), whereas correction with spherical lenses did not differ in
      driving safety from no correction (p = 0.118). Operational tests differentiated
      corrected from uncorrected performance for both spherical (p = 0.008) and toric
      (p = 0.011) lenses, but they were not sensitive enough to differentiate toric
      from spherical lens conditions. CONCLUSIONS: Given previous research showing that
      deficits in these tactical skills are predictive of future real-world collisions,
      these preliminary data suggest that correcting low to moderate astigmatism with
      toric lenses may be important to driving safety. Their merits relative to
      spherical lens correction require further investigation.
FAU - Cox, Daniel J
AU  - Cox DJ
AD  - *PhD daggerOD, PhD double daggerOD section signBA University of Virginia Health
      Sciences Center, Charlottesville, Virginia (DJC, TB, JHG, RJH); and Drs. Record, 
      Record & Adams, Optometrists, Charlottesville, Virginia (SR).
FAU - Banton, Thomas
AU  - Banton T
FAU - Record, Steven
AU  - Record S
FAU - Grabman, Jesse H
AU  - Grabman JH
FAU - Hawkins, Ronald J
AU  - Hawkins RJ
LA  - eng
PT  - Journal Article
PT  - Randomized Controlled Trial
PT  - Research Support, Non-U.S. Gov't
PL  - United States
TA  - Optom Vis Sci
JT  - Optometry and vision science : official publication of the American Academy of
      Optometry
JID - 8904931
SB  - IM
MH  - Adult
MH  - Astigmatism/physiopathology/*therapy
MH  - *Automobile Driving
MH  - Computer Simulation
MH  - *Contact Lenses, Hydrophilic
MH  - Contrast Sensitivity/physiology
MH  - Cross-Over Studies
MH  - Female
MH  - Humans
MH  - Male
MH  - Myopia/therapy
MH  - Psychomotor Performance/*physiology
MH  - Single-Blind Method
MH  - Visual Acuity/physiology
MH  - Young Adult
EDAT- 2015/05/07 06:00
MHDA- 2015/07/15 06:00
CRDT- 2015/05/07 06:00
AID - 10.1097/OPX.0000000000000554 [doi]
AID - 00006324-201504000-00005 [pii]
PST - ppublish
SO  - Optom Vis Sci. 2015 Apr;92(4):404-11. doi: 10.1097/OPX.0000000000000554.

PMID- 25486264
OWN - NLM
STAT- MEDLINE
DA  - 20150209
DCOM- 20151104
IS  - 1743-9159 (Electronic)
IS  - 1743-9159 (Linking)
VI  - 13
DP  - 2015 Jan
TI  - An over-view of robot assisted surgery curricula and the status of their
      validation.
PG  - 115-23
LID - 10.1016/j.ijsu.2014.11.033 [doi]
LID - S1743-9191(14)00993-5 [pii]
AB  - INTRODUCTION: Robotic surgery is a rapidly expanding field. Thus far training for
      robotic techniques has been unstructured and the requirements are variable across
      various regions. Several projects are currently underway to develop a robotic
      surgery curriculum and are in various stages of validation. We aimed to outline
      the structures of available curricula, their process of development, validation
      status and current utilization. METHODS: We undertook a literature review of
      papers including the MeSH terms "Robotics" and "Education". When we had an
      overview of curricula in development, we searched recent conference abstracts to 
      gain up to date information. RESULTS: The main curricula are the FRS, the FSRS,
      the Canadian BSTC and the ERUS initiative. They are in various stages of
      validation and offer a mixture of theoretical and practical training, using both 
      physical and simulated models. DISCUSSION: Whilst the FSRS is based on tasks on
      the RoSS virtual reality simulator, FRS and BSTC are designed for use on
      simulators and the robot itself. The ERUS curricula benefits from a combination
      of dry lab, wet lab and virtual reality components, which may allow skills to be 
      more transferable to the OR as tasks are completed in several formats. Finally,
      the ERUS curricula includes the OR modular training programme as table assistant 
      and console surgeon. CONCLUSION: Curricula are a crucial step in global
      standardisation of training and certification of surgeons for robotic surgical
      procedures. Many curricula are in early stages of development and more work is
      needed in development and validation of these programmes before training can be
      standardised.
CI  - Copyright (c) 2014 Surgical Associates Ltd. Published by Elsevier Ltd. All rights
      reserved.
FAU - Fisher, Rebecca A
AU  - Fisher RA
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Dasgupta, Prokar
AU  - Dasgupta P
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Mottrie, Alex
AU  - Mottrie A
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Volpe, Alessandro
AU  - Volpe A
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Khan, Mohammed S
AU  - Khan MS
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Challacombe, Ben
AU  - Challacombe B
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
FAU - Ahmed, Kamran
AU  - Ahmed K
AD  - MRC Centre for Transplantation, King's College London, King's Health Partners,
      Department of Urology, Guy's Hospital, St Thomas Street, London SE1 9RT, UK.
      Electronic address: kamran.ahmed@kcl.ac.uk.
LA  - eng
PT  - Journal Article
PT  - Review
DEP - 20141206
PL  - England
TA  - Int J Surg
JT  - International journal of surgery (London, England)
JID - 101228232
SB  - IM
MH  - Clinical Competence
MH  - Computer Simulation
MH  - *Curriculum
MH  - Humans
MH  - Reproducibility of Results
MH  - Robotic Surgical Procedures/*education
MH  - Specialties, Surgical/*education
OTO - NOTNLM
OT  - Curriculum
OT  - Education
OT  - Robotic
OT  - Surgery
OT  - Training
OT  - Urology
EDAT- 2014/12/09 06:00
MHDA- 2015/11/05 06:00
CRDT- 2014/12/09 06:00
PHST- 2014/08/04 [received]
PHST- 2014/11/20 [revised]
PHST- 2014/11/25 [accepted]
PHST- 2014/12/06 [aheadofprint]
AID - S1743-9191(14)00993-5 [pii]
AID - 10.1016/j.ijsu.2014.11.033 [doi]
PST - ppublish
SO  - Int J Surg. 2015 Jan;13:115-23. doi: 10.1016/j.ijsu.2014.11.033. Epub 2014 Dec 6.

PMID- 25361359
OWN - NLM
STAT- MEDLINE
DA  - 20141101
DCOM- 20151007
LR  - 20141213
IS  - 1938-2367 (Electronic)
IS  - 0147-7447 (Linking)
VI  - 37
IP  - 11
DP  - 2014 Nov
TI  - Emerging technology in surgical education: combining real-time augmented reality 
      and wearable computing devices.
PG  - 751-7
LID - 10.3928/01477447-20141023-05 [doi]
AB  - The authors describe the first surgical case adopting the combination of
      real-time augmented reality and wearable computing devices such as Google Glass
      (Google Inc, Mountain View, California). A 66-year-old man presented to their
      institution for a total shoulder replacement after 5 years of progressive right
      shoulder pain and decreased range of motion. Throughout the surgical procedure,
      Google Glass was integrated with the Virtual Interactive Presence and Augmented
      Reality system (University of Alabama at Birmingham, Birmingham, Alabama),
      enabling the local surgeon to interact with the remote surgeon within the local
      surgical field. Surgery was well tolerated by the patient and early surgical
      results were encouraging, with an improvement of shoulder pain and greater range 
      of motion. The combination of real-time augmented reality and wearable computing 
      devices such as Google Glass holds much promise in the field of surgery.
CI  - Copyright 2014, SLACK Incorporated.
FAU - Ponce, Brent A
AU  - Ponce BA
FAU - Menendez, Mariano E
AU  - Menendez ME
FAU - Oladeji, Lasun O
AU  - Oladeji LO
FAU - Fryberger, Charles T
AU  - Fryberger CT
FAU - Dantuluri, Phani K
AU  - Dantuluri PK
LA  - eng
PT  - Case Reports
PT  - Journal Article
PL  - United States
TA  - Orthopedics
JT  - Orthopedics
JID - 7806107
SB  - IM
MH  - Aged
MH  - Cooperative Behavior
MH  - Humans
MH  - Male
MH  - Microsurgery/methods
MH  - Shoulder Impingement Syndrome/radiography/*surgery
MH  - *Telemedicine
MH  - *User-Computer Interface
EDAT- 2014/11/02 06:00
MHDA- 2015/10/08 06:00
CRDT- 2014/11/01 06:00
PHST- 2013/12/04 [received]
PHST- 2014/02/20 [accepted]
AID - 10.3928/01477447-20141023-05 [doi]
PST - ppublish
SO  - Orthopedics. 2014 Nov;37(11):751-7. doi: 10.3928/01477447-20141023-05.

PMID- 24921542
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20140613
DCOM- 20150511
IS  - 1094-4087 (Electronic)
IS  - 1094-4087 (Linking)
VI  - 22
IP  - 11
DP  - 2014 Jun 2
TI  - A 3D integral imaging optical see-through head-mounted display.
PG  - 13484-91
LID - 10.1364/OE.22.013484 [doi]
AB  - An optical see-through head-mounted display (OST-HMD), which enables optical
      superposition of digital information onto the direct view of the physical world
      and maintains see-through vision to the real world, is a vital component in an
      augmented reality (AR) system. A key limitation of the state-of-the-art OST-HMD
      technology is the well-known accommodation-convergence mismatch problem caused by
      the fact that the image source in most of the existing AR displays is a 2D flat
      surface located at a fixed distance from the eye. In this paper, we present an
      innovative approach to OST-HMD designs by combining the recent advancement of
      freeform optical technology and microscopic integral imaging (micro-InI) method. 
      A micro-InI unit creates a 3D image source for HMD viewing optics, instead of a
      typical 2D display surface, by reconstructing a miniature 3D scene from a large
      number of perspective images of the scene. By taking advantage of the emerging
      freeform optical technology, our approach will result in compact, lightweight,
      goggle-style AR display that is potentially less vulnerable to the
      accommodation-convergence discrepancy problem and visual fatigue. A
      proof-of-concept prototype system is demonstrated, which offers a goggle-like
      compact form factor, non-obstructive see-through field of view, and true 3D
      virtual display.
FAU - Hua, Hong
AU  - Hua H
FAU - Javidi, Bahram
AU  - Javidi B
LA  - eng
PT  - Journal Article
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - United States
TA  - Opt Express
JT  - Optics express
JID - 101137103
EDAT- 2014/06/13 06:00
MHDA- 2014/06/13 06:01
CRDT- 2014/06/13 06:00
AID - 286531 [pii]
PST - ppublish
SO  - Opt Express. 2014 Jun 2;22(11):13484-91. doi: 10.1364/OE.22.013484.

PMID- 24892204
OWN - NLM
STAT- MEDLINE
DA  - 20140604
DCOM- 20141013
LR  - 20150601
IS  - 2152-2723 (Electronic)
VI  - 17
IP  - 6
DP  - 2014 Jun
TI  - Feasibility of articulated arm mounted Oculus Rift Virtual Reality goggles for
      adjunctive pain control during occupational therapy in pediatric burn patients.
PG  - 397-401
LID - 10.1089/cyber.2014.0058 [doi]
AB  - For daily burn wound care and therapeutic physical therapy skin stretching
      procedures, powerful pain medications alone are often inadequate. This
      feasibility study provides the first evidence that entering an immersive virtual 
      environment using very inexpensive ( approximately $400) wide field of view
      Oculus Rift Virtual Reality (VR) goggles can elicit a strong illusion of presence
      and reduce pain during VR. The patient was an 11-year-old male with severe
      electrical and flash burns on his head, shoulders, arms, and feet (36 percent
      total body surface area (TBSA), 27 percent TBSA were third-degree burns). He
      spent one 20-minute occupational therapy session with no VR, one with VR on day
      2, and a final session with no VR on day 3. His rating of pain intensity during
      therapy dropped from severely painful during no VR to moderately painful during
      VR. Pain unpleasantness dropped from moderately unpleasant during no VR to mildly
      unpleasant during VR. He reported going "completely inside the computer generated
      world", and had more fun during VR. Results are consistent with a growing
      literature showing reductions in pain during VR. Although case studies are
      scientifically inconclusive by nature, these preliminary results suggest that the
      Oculus Rift VR goggles merit more attention as a potential treatment for acute
      procedural pain of burn patients. Availability of inexpensive but highly
      immersive VR goggles would significantly improve cost effectiveness and increase 
      dissemination of VR pain distraction, making VR available to many more patients, 
      potentially even at home, for pain control as well as a wide range of other VR
      therapy applications. This is the first clinical data on PubMed to show the use
      of Oculus Rift for any medical application.
FAU - Hoffman, Hunter G
AU  - Hoffman HG
AD  - 1 Human Photonics Laboratory, Mechanical Engineering, University of Washington , 
      Seattle, Washington.
FAU - Meyer, Walter J 3rd
AU  - Meyer WJ 3rd
FAU - Ramirez, Maribel
AU  - Ramirez M
FAU - Roberts, Linda
AU  - Roberts L
FAU - Seibel, Eric J
AU  - Seibel EJ
FAU - Atzori, Barbara
AU  - Atzori B
FAU - Sharar, Sam R
AU  - Sharar SR
FAU - Patterson, David R
AU  - Patterson DR
LA  - eng
GR  - 1 R01AR054115-01A1/AR/NIAMS NIH HHS/United States
GR  - 2 R01 GM042725-17/GM/NIGMS NIH HHS/United States
PT  - Case Reports
PT  - Evaluation Studies
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
PL  - United States
TA  - Cyberpsychol Behav Soc Netw
JT  - Cyberpsychology, behavior and social networking
JID - 101528721
SB  - IM
MH  - Burns/*therapy
MH  - Child
MH  - Eye Protective Devices
MH  - Feasibility Studies
MH  - Female
MH  - Humans
MH  - Male
MH  - Occupational Therapy/*instrumentation
MH  - Pain Management/*methods
MH  - Virtual Reality Exposure Therapy/*instrumentation
PMC - PMC4043256
OID - NLM: PMC4043256
EDAT- 2014/06/04 06:00
MHDA- 2014/10/14 06:00
CRDT- 2014/06/04 06:00
AID - 10.1089/cyber.2014.0058 [doi]
PST - ppublish
SO  - Cyberpsychol Behav Soc Netw. 2014 Jun;17(6):397-401. doi:
      10.1089/cyber.2014.0058.

PMID- 24875655
OWN - NLM
STAT- MEDLINE
DA  - 20150124
DCOM- 20150730
IS  - 1861-6429 (Electronic)
IS  - 1861-6410 (Linking)
VI  - 10
IP  - 2
DP  - 2015 Feb
TI  - Tissue surface information for intraoperative incision planning and focus
      adjustment in laser surgery.
PG  - 171-81
LID - 10.1007/s11548-014-1077-x [doi]
AB  - PURPOSE: Introducing computational methods to laser surgery are an emerging
      field. Focusing on endoscopic laser interventions, a novel approach is presented 
      to enhance intraoperative incision planning and laser focusing by means of tissue
      surface information obtained by stereoscopic vision. METHODS: Tissue surface is
      estimated with stereo-based methods using nonparametric image transforms.
      Subsequently, laser-to-camera registration is obtained by ablating a pattern on
      tissue substitutes and performing a principle component analysis for precise
      laser axis estimation. Furthermore, a virtual laser view is computed utilizing
      trifocal transfer. Depth-based laser focus adaptation is integrated into a custom
      experimental laser setup in order to achieve optimal ablation morphology.
      Experimental validation is conducted on tissue substitutes and ex vivo animal
      tissue. RESULTS: Laser-to-camera registration gives an error between planning and
      ablation of less than 0.2 mm. As a result, the laser workspace can accurately be 
      highlighted within the live views and incision planning can directly be
      performed. Experiments related to laser focus adaptation demonstrate that
      ablation geometry can be kept almost uniform within a depth range of 7.9 mm,
      whereas cutting quality significantly decreases when the laser is defocused.
      CONCLUSIONS: An automatic laser focus adjustment on tissue surfaces based on
      stereoscopic scene information is feasible and has the potential to become an
      effective methodology for optimal ablation. Laser-to-camera registration
      facilitates advanced surgical planning for prospective user interfaces and
      augmented reality extensions.
FAU - Schoob, Andreas
AU  - Schoob A
AD  - Institute of Mechatronic Systems, Leibniz Universitat Hannover, 30167 , Hanover, 
      Germany, andreas.schoob@imes.uni-hannover.de.
FAU - Kundrat, Dennis
AU  - Kundrat D
FAU - Kleingrothe, Lukas
AU  - Kleingrothe L
FAU - Kahrs, Luder A
AU  - Kahrs LA
FAU - Andreff, Nicolas
AU  - Andreff N
FAU - Ortmaier, Tobias
AU  - Ortmaier T
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20140530
PL  - Germany
TA  - Int J Comput Assist Radiol Surg
JT  - International journal of computer assisted radiology and surgery
JID - 101499225
SB  - IM
MH  - Depth Perception
MH  - Endoscopy/*methods
MH  - Humans
MH  - Laser Therapy/*methods
MH  - *Lasers
MH  - Surgery, Computer-Assisted/*methods
EDAT- 2014/05/31 06:00
MHDA- 2015/08/01 06:00
CRDT- 2014/05/31 06:00
PHST- 2014/01/10 [received]
PHST- 2014/05/16 [accepted]
PHST- 2014/05/30 [aheadofprint]
AID - 10.1007/s11548-014-1077-x [doi]
PST - ppublish
SO  - Int J Comput Assist Radiol Surg. 2015 Feb;10(2):171-81. doi:
      10.1007/s11548-014-1077-x. Epub 2014 May 30.

PMID- 24801324
OWN - NLM
STAT- MEDLINE
DA  - 20140507
DCOM- 20151013
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 9
IP  - 5
DP  - 2014
TI  - A meta-analysis on the relationship between self-reported presence and anxiety in
      virtual reality exposure therapy for anxiety disorders.
PG  - e96144
LID - 10.1371/journal.pone.0096144 [doi]
AB  - In virtual reality exposure therapy (VRET) for anxiety disorders, sense of
      presence in the virtual environment is considered the principal mechanism that
      enables anxiety to be felt. Existing studies on the relation between sense of
      presence and level of anxiety, however, have yielded mixed results on the
      correlation between the two. In this meta-analysis, we reviewed publications on
      VRET for anxiety that included self-reported presence and anxiety. The
      comprehensive search of the literature identified 33 publications with a total of
      1196 participants. The correlation between self-reported sense of presence and
      anxiety was extracted and meta-analyzed. Potential moderators such as technology 
      characteristics, sample characteristics including age, gender and clinical
      status, disorder characteristics and study design characteristics such as
      measurements were also examined. The random effects analysis showed a medium
      effect size for the correlation between sense of presence and anxiety (r = .28;
      95% CI: 0.18-0.38). Moderation analyses revealed that the effect size of the
      correlation differed across different anxiety disorders, with a large effect size
      for fear of animals (r = .50; 95% CI: 0.30-0.66) and a no to small effect size
      for social anxiety disorder (r = .001; 95% CI: -0.19-0.19). Further, the
      correlation between anxiety and presence was stronger in studies with
      participants who met criteria for an anxiety disorder than in studies with a
      non-clinical population. Trackers with six degrees of freedom and displays with a
      larger field of view resulted in higher effect sizes, compared to trackers with
      three degrees of freedom and displays with a smaller field of view. In addition, 
      no difference in effect size was found for the type of presence measurement and
      the type of anxiety measurement. This meta-analysis confirms the positive
      relation between sense of presence and anxiety and demonstrates that this
      relation can be affected by various moderating factors.
FAU - Ling, Yun
AU  - Ling Y
AD  - Interactive Intelligence Group, Delft University of Technology, Delft, the
      Netherlands.
FAU - Nefs, Harold T
AU  - Nefs HT
AD  - Interactive Intelligence Group, Delft University of Technology, Delft, the
      Netherlands.
FAU - Morina, Nexhmedin
AU  - Morina N
AD  - Department of Clinical Psychology, University of Amsterdam, Amsterdam, the
      Netherlands.
FAU - Heynderickx, Ingrid
AU  - Heynderickx I
AD  - Human Technology Interaction Group, Eindhoven University of Technology,
      Eindhoven, the Netherlands; Visual Experiences Group, Philips Research
      Laboratories, Eindhoven, the Netherlands.
FAU - Brinkman, Willem-Paul
AU  - Brinkman WP
AD  - Interactive Intelligence Group, Delft University of Technology, Delft, the
      Netherlands.
LA  - eng
PT  - Journal Article
PT  - Meta-Analysis
PT  - Research Support, Non-U.S. Gov't
DEP - 20140506
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - Anxiety Disorders/*therapy
MH  - Humans
MH  - Self Report
MH  - *Virtual Reality Exposure Therapy
PMC - PMC4011738
OID - NLM: PMC4011738
EDAT- 2014/05/08 06:00
MHDA- 2015/10/16 06:00
CRDT- 2014/05/08 06:00
PHST- 2014 [ecollection]
PHST- 2013/10/17 [received]
PHST- 2014/04/03 [accepted]
PHST- 2014/05/06 [epublish]
AID - 10.1371/journal.pone.0096144 [doi]
AID - PONE-D-13-42501 [pii]
PST - epublish
SO  - PLoS One. 2014 May 6;9(5):e96144. doi: 10.1371/journal.pone.0096144. eCollection 
      2014.

PMID- 24619331
OWN - NLM
STAT- MEDLINE
DA  - 20140701
DCOM- 20141204
LR  - 20150801
IS  - 1432-2218 (Electronic)
IS  - 0930-2794 (Linking)
VI  - 28
IP  - 8
DP  - 2014 Aug
TI  - A comparison of NOTES transvaginal and laparoscopic cholecystectomy procedures
      based upon task analysis.
PG  - 2443-51
LID - 10.1007/s00464-014-3495-9 [doi]
AB  - BACKGROUND: A virtual reality-based simulator for natural orifice translumenal
      endoscopic surgery (NOTES) procedures may be used for training and discovery of
      new tools and procedures. Our previous study (Sankaranarayanan et al. in Surg
      Endosc 27:1607-1616, 2013) shows that developing such a simulator for the
      transvaginal cholecystectomy procedure using a rigid endoscope will have the most
      impact on the field. However, prior to developing such a simulator, a thorough
      task analysis is necessary to determine the most important phases, tasks, and
      subtasks of this procedure. METHODS: 19 rigid endoscope transvaginal hybrid NOTES
      cholecystectomy procedures and 11 traditional laparoscopic procedures have been
      recorded and de-identified prior to analysis. Hierarchical task analysis was
      conducted for the rigid endoscope transvaginal NOTES cholecystectomy. A time
      series analysis was conducted to evaluate the performance of the transvaginal
      NOTES and laparoscopic cholecystectomy procedures. Finally, a comparison of
      electrosurgery-based errors was performed by two independent qualified personnel.
      RESULTS: The most time-consuming tasks for both laparoscopic and NOTES
      cholecystectomy are removing areolar and connective tissue surrounding the
      gallbladder, exposing Calot's triangle, and dissecting the gallbladder off the
      liver bed with electrosurgery. There is a positive correlation of performance
      time between the removal of areolar and connective tissue and electrosurgery
      dissection tasks in NOTES (r = 0.415) and laparoscopic cholecystectomy (r =
      0.684) with p < 0.10. During the electrosurgery task, the NOTES procedures had
      fewer errors related to lack of progress in gallbladder removal. Contrarily,
      laparoscopic procedures had fewer errors due to the instrument being out of the
      camera view. CONCLUSION: A thorough task analysis and video-based quantification 
      of NOTES cholecystectomy has identified the most time-consuming tasks. A
      comparison of the surgical errors during electrosurgery gallbladder dissection
      establishes that the NOTES procedure, while still new, is not inferior to the
      established laparoscopic procedure.
FAU - Nemani, Arun
AU  - Nemani A
AD  - Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer
      Polytechnic Institute, Troy, NY, USA, nemana@rpi.edu.
FAU - Sankaranarayanan, Ganesh
AU  - Sankaranarayanan G
FAU - Olasky, Jaisa S
AU  - Olasky JS
FAU - Adra, Souheil
AU  - Adra S
FAU - Roberts, Kurt E
AU  - Roberts KE
FAU - Panait, Lucian
AU  - Panait L
FAU - Schwaitzberg, Steven D
AU  - Schwaitzberg SD
FAU - Jones, Daniel B
AU  - Jones DB
FAU - De, Suvranu
AU  - De S
LA  - eng
GR  - 1R01EB009362/EB/NIBIB NIH HHS/United States
GR  - 2R01EB00580/EB/NIBIB NIH HHS/United States
GR  - 5R01EB010037/EB/NIBIB NIH HHS/United States
GR  - R01 EB005807/EB/NIBIB NIH HHS/United States
GR  - R01 EB009362/EB/NIBIB NIH HHS/United States
GR  - R01 EB010037/EB/NIBIB NIH HHS/United States
GR  - R01 EB014305/EB/NIBIB NIH HHS/United States
PT  - Comparative Study
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
DEP - 20140312
PL  - Germany
TA  - Surg Endosc
JT  - Surgical endoscopy
JID - 8806653
SB  - IM
MH  - Cholecystectomy, Laparoscopic/*methods
MH  - Electrosurgery
MH  - Endoscopes
MH  - Female
MH  - Gallbladder/*surgery
MH  - Humans
MH  - *Interrupted Time Series Analysis
MH  - Intraoperative Complications
MH  - Natural Orifice Endoscopic Surgery/*methods
MH  - Operative Time
MH  - Vagina/*surgery
MH  - Videotape Recording
PMC - PMC4077992
MID - NIHMS574840
OID - NLM: NIHMS574840
OID - NLM: PMC4077992
EDAT- 2014/03/13 06:00
MHDA- 2014/12/15 06:00
CRDT- 2014/03/13 06:00
PHST- 2013/08/06 [received]
PHST- 2014/02/17 [accepted]
PHST- 2014/03/12 [aheadofprint]
AID - 10.1007/s00464-014-3495-9 [doi]
PST - ppublish
SO  - Surg Endosc. 2014 Aug;28(8):2443-51. doi: 10.1007/s00464-014-3495-9. Epub 2014
      Mar 12.

PMID- 24434221
OWN - NLM
STAT- MEDLINE
DA  - 20140117
DCOM- 20140922
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
VI  - 20
IP  - 3
DP  - 2014 Mar
TI  - Attributes of subtle cues for facilitating visual search in augmented reality.
PG  - 404-12
LID - 10.1109/TVCG.2013.241 [doi]
AB  - Goal-oriented visual search is performed when a person intentionally seeks a
      target in the visual environment. In augmented reality (AR) environments, visual 
      search can be facilitated by augmenting virtual cues in the person's field of
      view. Traditional use of explicit AR cues can potentially degrade visual search
      performance due to the creation of distortions in the scene. An alternative to
      explicit cueing, known as subtle cueing, has been proposed as a clutter-neutral
      method to enhance visual search in video-see-through AR. However, the effects of 
      subtle cueing are still not well understood, and more research is required to
      determine the optimal methods of applying subtle cueing in AR. We performed two
      experiments to investigate the variables of scene clutter, subtle cue opacity,
      size, and shape on visual search performance. We introduce a novel method of
      experimentally manipulating the scene clutter variable in a natural scene while
      controlling for other variables. The findings provide supporting evidence for the
      subtlety of the cue, and show that the clutter conditions of the scene can be
      used both as a global classifier, as well as a local performance measure.
FAU - Lu, Weiquan
AU  - Lu W
AD  - National University of Singapore, Singapore.
FAU - Duh, Henry Been-Lirn
AU  - Duh HB
AD  - National University of Singapore, Singapore.
FAU - Feiner, Steven
AU  - Feiner S
AD  - Columbia University, New York.
FAU - Zhao, Qi
AU  - Zhao Q
AD  - National University of Singapore, Singapore.
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
MH  - Adult
MH  - *Cues
MH  - Female
MH  - Humans
MH  - Male
MH  - Photic Stimulation/methods
MH  - *User-Computer Interface
MH  - Visual Perception/*physiology
MH  - Young Adult
EDAT- 2014/01/18 06:00
MHDA- 2014/09/23 06:00
CRDT- 2014/01/18 06:00
AID - 10.1109/TVCG.2013.241 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2014 Mar;20(3):404-12. doi: 10.1109/TVCG.2013.241.

PMID- 24132469
OWN - NLM
STAT- In-Process
DA  - 20150408
IS  - 1553-3514 (Electronic)
IS  - 1553-3506 (Linking)
VI  - 21
IP  - 3
DP  - 2014 Jun
TI  - A multicenter prospective cohort study on camera navigation training for key user
      groups in minimally invasive surgery.
PG  - 312-9
LID - 10.1177/1553350613505714 [doi]
AB  - BACKGROUND: Untrained laparoscopic camera assistants in minimally invasive
      surgery (MIS) may cause suboptimal view of the operating field, thereby
      increasing risk for errors. Camera navigation is often performed by the least
      experienced member of the operating team, such as inexperienced surgical
      residents, operating room nurses, and medical students. The operating room nurses
      and medical students are currently not included as key user groups in structured 
      laparoscopic training programs. A new virtual reality laparoscopic camera
      navigation (LCN) module was specifically developed for these key user groups.
      METHODS: This multicenter prospective cohort study assesses face validity and
      construct validity of the LCN module on the Simendo virtual reality simulator.
      Face validity was assessed through a questionnaire on resemblance to reality and 
      perceived usability of the instrument among experts and trainees. Construct
      validity was assessed by comparing scores of groups with different levels of
      experience on outcome parameters of speed and movement proficiency. RESULTS: The 
      results obtained show uniform and positive evaluation of the LCN module among
      expert users and trainees, signifying face validity. Experts and intermediate
      experience groups performed significantly better in task time and camera
      stability during three repetitions, compared to the less experienced user groups 
      (P < .007). Comparison of learning curves showed significant improvement of
      proficiency in time and camera stability for all groups during three repetitions 
      (P < .007). CONCLUSION: The results of this study show face validity and
      construct validity of the LCN module. The module is suitable for use in training 
      curricula for operating room nurses and novice surgical trainees, aimed at
      improving team performance in minimally invasive surgery.
CI  - (c) The Author(s) 2013.
FAU - Graafland, Maurits
AU  - Graafland M
AD  - Department of Surgery, Academic Medical Centre, Amsterdam, The Netherlands.
FAU - Bok, Kiki
AU  - Bok K
AD  - Division of Woman & Baby, Department of Reproductive Medicine and Gynaecology,
      University Medical Centre Utrecht, Utrecht, The Netherlands.
FAU - Schreuder, Henk W R
AU  - Schreuder HW
AD  - Division of Woman & Baby, Department of Reproductive Medicine and Gynaecology,
      University Medical Centre Utrecht, Utrecht, The Netherlands.
FAU - Schijven, Marlies P
AU  - Schijven MP
AD  - Department of Surgery, Academic Medical Centre, Amsterdam, The Netherlands
      m.p.schijven@amc.uva.nl.
LA  - eng
PT  - Journal Article
DEP - 20131016
PL  - United States
TA  - Surg Innov
JT  - Surgical innovation
JID - 101233809
SB  - IM
OTO - NOTNLM
OT  - ergonomics
OT  - evidence-based medicine/surgery
OT  - human factors study
OT  - simulation
OT  - surgical education
EDAT- 2013/10/18 06:00
MHDA- 2013/10/18 06:00
CRDT- 2013/10/18 06:00
PHST- 2013/10/16 [aheadofprint]
AID - 1553350613505714 [pii]
AID - 10.1177/1553350613505714 [doi]
PST - ppublish
SO  - Surg Innov. 2014 Jun;21(3):312-9. doi: 10.1177/1553350613505714. Epub 2013 Oct
      16.

PMID- 24101338
OWN - NLM
STAT- Publisher
DA  - 20131008
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
DP  - 2013 Oct 3
TI  - Attributes of Subtle Cues for Facilitating Visual Search in Augmented Reality.
AB  - Goal-oriented Visual Search is performed when a person intentionally seeks a
      target in the visual environment. In Augmented Reality (AR) environments, Visual 
      Search can be facilitated by augmenting virtual cues in the person's field of
      view. Traditional use of explicit AR cues can potentially degrade Visual Search
      performance due to the creation of distortions in the scene. An alternative to
      explicit cueing, known as Subtle Cueing, has been proposed as a clutter-neutral
      method to enhance Visual Search in video-see-through AR. However, the effects of 
      Subtle Cueing are still not well understood, and more research is required to
      determine the optimal methods of applying Subtle Cueing in AR. We performed two
      experiments to investigate the variables of scene clutter, subtle cue opacity,
      size and shape on Visual Search performance. We introduced a novel method of
      experimentally manipulating the scene clutter variable in a natural scene while
      controlling for other variables. The findings provide supporting evidence for the
      subtlety of the cue, and show that the clutter conditions of the scene can be
      used both as a global classifier as well as a local performance measure.
FAU - Lu, Weiquan
AU  - Lu W
AD  - National University of Singapore, Singapore.
FAU - Duh, Henry Been-Lirn
AU  - Duh HB
FAU - Feiner, Steven
AU  - Feiner S
FAU - Zhao, Qi
AU  - Zhao Q
LA  - ENG
PT  - JOURNAL ARTICLE
DEP - 20131003
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
EDAT- 2013/10/09 06:00
MHDA- 2013/10/09 06:00
CRDT- 2013/10/09 06:00
AID - 0C63A901-D405-4CB8-A5D4-3123BE6BEB1C [doi]
PST - aheadofprint
SO  - IEEE Trans Vis Comput Graph. 2013 Oct 3.

PMID- 23649730
OWN - NLM
STAT- MEDLINE
DA  - 20131101
DCOM- 20141029
IS  - 1861-6429 (Electronic)
IS  - 1861-6410 (Linking)
VI  - 8
IP  - 6
DP  - 2013 Nov
TI  - Projection-based visual guidance for robot-aided RF needle insertion.
PG  - 1015-25
LID - 10.1007/s11548-013-0897-4 [doi]
AB  - PURPOSE: The use of projector-based augmented reality (AR) in surgery may enable 
      surgeons to directly view anatomical models and surgical data from the patient's 
      surface (skin). It has the advantages of a consistent viewing focus on the
      patient, an extended field of view and augmented interaction. This paper presents
      an AR guidance mechanism with a projector-camera system to provide the surgeon
      with direct visual feedback for supervision of robotic needle insertion in
      radiofrequency (RF) ablation treatment. METHODS: The registration of target organ
      models to specific positions on the patient body is performed using a
      surface-matching algorithm and point-based registration. An algorithm based on
      the extended Kalman filter and spatial transformation is used to intraoperatively
      compute the virtual needle's depth in the patient's body for AR display. RESULTS:
      Experiments of this AR system on a mannequin were conducted to evaluate AR
      visualization and accuracy of virtual RF needle insertion. The average accuracy
      of 1.86 mm for virtual needle insertion met the clinical requirement of 2 mm or
      better. The feasibility of augmented interaction with a surgical robot using the 
      proposed open AR interface with active visual feedback was demonstrated.
      CONCLUSIONS: The experimental results demonstrate that this guidance system is
      effective in assisting a surgeon to perform a robot-assisted radiofrequency
      ablation procedure. The novelty of the work lies in establishing a navigational
      procedure for percutaneous surgical augmented intervention integrating a
      projection-based AR guidance and robotic implementation for surgical needle
      insertion.
FAU - Wen, Rong
AU  - Wen R
AD  - Department of Mechanical Engineering, National University of Singapore,
      Singapore, 117576, Singapore, g0801390@nus.edu.sg.
FAU - Chui, Chee-Kong
AU  - Chui CK
FAU - Ong, Sim-Heng
AU  - Ong SH
FAU - Lim, Kah-Bin
AU  - Lim KB
FAU - Chang, Stephen Kin-Yong
AU  - Chang SK
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20130507
PL  - Germany
TA  - Int J Comput Assist Radiol Surg
JT  - International journal of computer assisted radiology and surgery
JID - 101499225
SB  - IM
MH  - Catheter Ablation/instrumentation/*methods
MH  - Humans
MH  - Models, Anatomic
MH  - *Needles
MH  - Robotics/*instrumentation
MH  - Software
MH  - Surgery, Computer-Assisted/instrumentation/*methods
EDAT- 2013/05/08 06:00
MHDA- 2014/10/30 06:00
CRDT- 2013/05/08 06:00
PHST- 2013/01/07 [received]
PHST- 2013/04/22 [accepted]
PHST- 2013/05/07 [aheadofprint]
AID - 10.1007/s11548-013-0897-4 [doi]
PST - ppublish
SO  - Int J Comput Assist Radiol Surg. 2013 Nov;8(6):1015-25. doi:
      10.1007/s11548-013-0897-4. Epub 2013 May 7.

PMID- 23562204
OWN - NLM
STAT- MEDLINE
DA  - 20130408
DCOM- 20131028
IS  - 1945-8932 (Electronic)
IS  - 1945-8932 (Linking)
VI  - 27
IP  - 2
DP  - 2013 Mar-Apr
TI  - Toward photorealism in endoscopic sinus surgery simulation.
PG  - 138-43
LID - 10.2500/ajra.2013.27.3861 [doi]
AB  - BACKGROUND: Endoscopic sinus surgery (ESS) is the surgical standard treatment for
      chronic rhinitis/rhinosinusitis and nasal polyposis. There is a reported
      complication rate of 5-10% associated with this type of surgery. Simulation has
      been advocated as a means to improve surgical training and minimize the rates of 
      complication and medical error. This study aimed to show how a virtual reality
      ESS simulator was developed, with particular emphasis on achieving satisfactory
      photorealism and surgical verisimilitude. METHODS: Sinus computed tomography
      scans were processed to create a triangle-based three-dimensional mesh model;
      this was incorporated into a spring-damper model of thousands of interconnected
      nodes, which is allowed to deform in response to user interactions. Dual haptic
      handpiece devices were programmed to simulate an endoscope and various surgical
      instruments. Textures and lighting effects were added to the mesh model to
      provide an accurate representation of the surgical field. Effects such as
      vasoconstriction in response to "virtual" decongestant were added. RESULTS: The
      final simulated endoscopic view of the sinuses accurately simulates the moist and
      glossy appearance of the sinuses. The interactive tissue simulation system
      enables the user to interactively cut and remove tissue while receiving accurate 
      haptic feedback. A working prototype of the simulator has been developed that
      leverages recent advances in computer hardware to deliver a realistic user
      experience, both visually and haptically. CONCLUSION: This new computer-based
      training tool for practicing ESS provides a risk-free environment for surgical
      trainees to practice and develop core skills. The novel use of customized
      precision force feedback (haptic) devices enables trainees to use movements
      during training that closely mimic those used during the actual procedure, which 
      we anticipate will improve learning, retention, and recall.
FAU - Ruthenbeck, Greg S
AU  - Ruthenbeck GS
AD  - School of Science, Engineering and Mathematics, Flinders University, South
      Australia, Australia.
FAU - Hobson, Jonathan
AU  - Hobson J
FAU - Carney, A Simon
AU  - Carney AS
FAU - Sloan, Steve
AU  - Sloan S
FAU - Sacks, Raymond
AU  - Sacks R
FAU - Reynolds, Karen J
AU  - Reynolds KJ
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - United States
TA  - Am J Rhinol Allergy
JT  - American journal of rhinology & allergy
JID - 101490775
SB  - IM
MH  - Chronic Disease
MH  - Computer Simulation
MH  - Computer-Assisted Instruction
MH  - Endoscopy/adverse effects/*methods
MH  - Humans
MH  - Imaging, Three-Dimensional
MH  - Nasal Polyps/radiography/*surgery
MH  - Paranasal Sinuses/radiography/*surgery
MH  - Photography
MH  - Postoperative Complications/prevention & control
MH  - Professional Practice
MH  - Rhinitis/radiography/*surgery
MH  - Sinusitis/radiography/*surgery
MH  - Software
MH  - Tomography, X-Ray Computed
MH  - User-Computer Interface
EDAT- 2013/04/09 06:00
MHDA- 2013/10/29 06:00
CRDT- 2013/04/09 06:00
AID - 10.2500/ajra.2013.27.3861 [doi]
PST - ppublish
SO  - Am J Rhinol Allergy. 2013 Mar-Apr;27(2):138-43. doi: 10.2500/ajra.2013.27.3861.

PMID- 23400197
OWN - NLM
STAT- MEDLINE
DA  - 20130212
DCOM- 20130702
IS  - 0926-9630 (Print)
IS  - 0926-9630 (Linking)
VI  - 184
DP  - 2013
TI  - Laparoscopic surgery simulator using first person view and guidance force.
PG  - 431-5
AB  - In general, minimally invasive surgery is seen as the most difficult surgery
      because there is limited field of view with an endoscope and force sensation from
      surgical instruments such as forceps is poor. Especially in early clinical
      education for medical students, a virtual reality surgical simulator would be an 
      effective tool. In this paper, we propose a visuohaptic surgery training system
      for laparoscopical techniques. We recorded a video from a first person point of
      view of the instructor (expert). And we also recorded operation information (i.e.
      trajectory) of surgical instruments of the instructor. Then, we displayed the
      recorded video and the guidance force to trainees. We constructed a prototype
      surgery training system and the effectiveness of our approach was confirmed.
FAU - Tagawa, Kazuyoshi
AU  - Tagawa K
AD  - Ritsumeikan University. tagawa@tagawa.info
FAU - Tanaka, Hiromi T
AU  - Tanaka HT
FAU - Kurumi, Yoshimasa
AU  - Kurumi Y
FAU - Komori, Masaru
AU  - Komori M
FAU - Morikawa, Shigehiro
AU  - Morikawa S
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - Netherlands
TA  - Stud Health Technol Inform
JT  - Studies in health technology and informatics
JID - 9214582
SB  - T
MH  - Computer-Assisted Instruction/*methods
MH  - *Expert Systems
MH  - Imaging, Three-Dimensional/*methods
MH  - Laparoscopy/*education/*methods
MH  - Surgery, Computer-Assisted/*methods
MH  - Teaching/*methods
MH  - *User-Computer Interface
EDAT- 2013/02/13 06:00
MHDA- 2013/07/03 06:00
CRDT- 2013/02/13 06:00
PST - ppublish
SO  - Stud Health Technol Inform. 2013;184:431-5.

PMID- 23395984
OWN - NLM
STAT- MEDLINE
DA  - 20130314
DCOM- 20130415
LR  - 20141103
IS  - 1476-4687 (Electronic)
IS  - 0028-0836 (Linking)
VI  - 495
IP  - 7440
DP  - 2013 Mar 14
TI  - Membrane potential dynamics of grid cells.
PG  - 199-204
LID - 10.1038/nature11973 [doi]
AB  - During navigation, grid cells increase their spike rates in firing fields
      arranged on a markedly regular triangular lattice, whereas their spike timing is 
      often modulated by theta oscillations. Oscillatory interference models of grid
      cells predict theta amplitude modulations of membrane potential during firing
      field traversals, whereas competing attractor network models predict slow
      depolarizing ramps. Here, using in vivo whole-cell recordings, we tested these
      models by directly measuring grid cell intracellular potentials in mice running
      along linear tracks in virtual reality. Grid cells had large and reproducible
      ramps of membrane potential depolarization that were the characteristic signature
      tightly correlated with firing fields. Grid cells also demonstrated intracellular
      theta oscillations that influenced their spike timing. However, the properties of
      theta amplitude modulations were not consistent with the view that they determine
      firing field locations. Our results support cellular and network mechanisms in
      which grid fields are produced by slow ramps, as in attractor models, whereas
      theta oscillations control spike timing.
FAU - Domnisoru, Cristina
AU  - Domnisoru C
AD  - Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey
      08544, USA.
FAU - Kinkhabwala, Amina A
AU  - Kinkhabwala AA
FAU - Tank, David W
AU  - Tank DW
LA  - eng
GR  - 1R37NS081242-01/NS/NINDS NIH HHS/United States
GR  - 5R01MH083686-04/MH/NIMH NIH HHS/United States
GR  - 5RC1NS068148-02/NS/NINDS NIH HHS/United States
GR  - F32NS070514-01A1/NS/NINDS NIH HHS/United States
GR  - R37 NS081242/NS/NINDS NIH HHS/United States
GR  - RC1 NS068148/NS/NINDS NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, U.S. Gov't, Non-P.H.S.
DEP - 20130210
PL  - England
TA  - Nature
JT  - Nature
JID - 0410462
SB  - IM
EIN - Nature. 2013 Dec 19;504(7480):470
MH  - Action Potentials/physiology
MH  - Animals
MH  - Entorhinal Cortex/*cytology
MH  - Membrane Potentials/*physiology
MH  - Mice
MH  - Mice, Inbred C57BL
MH  - Models, Neurological
MH  - Patch-Clamp Techniques
MH  - Space Perception
MH  - Theta Rhythm
PMC - PMC4099005
MID - NIHMS457568
OID - NLM: NIHMS457568
OID - NLM: PMC4099005
EDAT- 2013/02/12 06:00
MHDA- 2013/04/16 06:00
CRDT- 2013/02/12 06:00
PHST- 2012/08/19 [received]
PHST- 2013/02/01 [accepted]
PHST- 2013/02/10 [aheadofprint]
AID - nature11973 [pii]
AID - 10.1038/nature11973 [doi]
PST - ppublish
SO  - Nature. 2013 Mar 14;495(7440):199-204. doi: 10.1038/nature11973. Epub 2013 Feb
      10.

PMID- 23389366
OWN - NLM
STAT- MEDLINE
DA  - 20130207
DCOM- 20130329
IS  - 2038-2529 (Electronic)
IS  - 0300-8916 (Linking)
VI  - 98
IP  - 6
DP  - 2012 Nov
TI  - Percutaneous computed tomography-guided lung biopsies: preliminary results using 
      an augmented reality navigation system.
PG  - 775-82
LID - 10.1700/1217.13503 [doi]
AB  - AIMS AND BACKGROUND: "Augmented reality" is a technique to create a composite
      view by augmenting the real intervention field, visualized by the doctor, with
      additional information coming from a virtual volume generated using computed
      tomography (CT), magnetic resonance or ultrasound images previously acquired from
      the same patient. In the present study we verified the accuracy and validated the
      clinical use of an augmented reality navigation system produced to perform
      percutaneous CT-guided lung biopsies. METHODS: One hundred and eighty consecutive
      patients with solitary parenchymal lung lesions, enrolled using a nonrandom
      enrollment system, underwent percutaneous CT-guided aspiration and core biopsy
      using a traditional technique (group C, 90 patients) and navigation system
      assistance (group S, 90 patients). For each patient we recorded the largest
      lesion diameter, procedure time, overall number of CT scans, radiation dose, and 
      complications. The entire experimental project was evaluated and approved by the 
      local institutional review board (ethics committee). RESULTS: Each procedure was 
      concluded successfully and a pathological diagnosis was reached in 96% of cases
      in group S and 90% of cases in group C. Procedure time, overall number of CT
      scans and incident x-ray radiation dose (CTDIvol) were significantly reduced in
      navigation system-assisted procedures (P <0.001; z = 5.64) compared with
      traditional CT-guided procedures. The percentage of procedural complications was 
      14% in group S and 17% in group C. CONCLUSION: The augmented reality navigation
      system used in this study was a highly safe, technically reliable and effective
      support tool in percutaneous CT-guided lung biopsy, allowing to shorten the
      procedure time and reduce the incident x-ray radiation dose to patients and the
      rate of insufficient specimens. Furthermore, it has the potential to increase the
      number of procedures executed in the allocated time without increasing the number
      of complications.
FAU - Grasso, Rosario Francesco
AU  - Grasso RF
AD  - Universita Campus Bio-Medico, Via Alvaro Del Portillo 200, Rome, Italy.
FAU - Luppi, Giacomo
AU  - Luppi G
FAU - Cazzato, Roberto Luigi
AU  - Cazzato RL
FAU - Faiella, Eliodoro
AU  - Faiella E
FAU - D'Agostino, Francesco
AU  - D'Agostino F
FAU - Beomonte Zobel, Daniela
AU  - Beomonte Zobel D
FAU - De Lena, Mario
AU  - De Lena M
LA  - eng
PT  - Clinical Trial
PT  - Journal Article
PT  - Validation Studies
PL  - Italy
TA  - Tumori
JT  - Tumori
JID - 0111356
SB  - IM
MH  - Adult
MH  - Aged
MH  - Biopsy, Needle/instrumentation/*methods
MH  - Female
MH  - Humans
MH  - Italy
MH  - Lung/*pathology/radiography/surgery
MH  - Lung Neoplasms/*diagnosis/pathology/radiography/surgery
MH  - Male
MH  - Middle Aged
MH  - Operative Time
MH  - Prospective Studies
MH  - Radiation Dosage
MH  - Surgery, Computer-Assisted/instrumentation/*methods
MH  - *Tomography, X-Ray Computed/instrumentation
MH  - *User-Computer Interface
EDAT- 2013/02/08 06:00
MHDA- 2013/03/30 06:00
CRDT- 2013/02/08 06:00
AID - 10.1700/1217.13503 [doi]
PST - ppublish
SO  - Tumori. 2012 Nov;98(6):775-82. doi: 10.1700/1217.13503.

PMID- 23260468
OWN - NLM
STAT- MEDLINE
DA  - 20130125
DCOM- 20130806
IS  - 1879-0445 (Electronic)
IS  - 0960-9822 (Linking)
VI  - 23
IP  - 2
DP  - 2013 Jan 21
TI  - Parietal cortex codes for egocentric space beyond the field of view.
PG  - 177-82
LID - 10.1016/j.cub.2012.11.060 [doi]
LID - S0960-9822(12)01440-6 [pii]
AB  - Our subjective experience links covert visual and egocentric spatial attention
      seamlessly. However, the latter can extend beyond the visual field, covering all 
      directions relative to our body. In contrast to visual representations, little is
      known about unseen egocentric representations in the healthy brain. Parietal
      cortex appears to be involved in both, because lesions in it can lead to deficits
      in visual attention, but also to a disorder of egocentric spatial awareness,
      known as hemispatial neglect. Here, we used a novel virtual reality paradigm to
      probe our participants' egocentric surrounding during fMRI recordings. We found
      that egocentric unseen space was represented by patterns of voxel activity in
      parietal cortex, independent of visual information. Intriguingly, the best
      decoding performances corresponded to brain areas associated with visual covert
      attention and reaching, as well as to lesion sites associated with spatial
      neglect.
CI  - Copyright (c) 2013 Elsevier Ltd. All rights reserved.
FAU - Schindler, Andreas
AU  - Schindler A
AD  - Vision and Cognition Lab, Centre for Integrative Neuroscience, University of
      Tubingen, Otfried-Muller-Strasse 25, 72076 Tubingen, Germany.
FAU - Bartels, Andreas
AU  - Bartels A
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20121220
PL  - England
TA  - Curr Biol
JT  - Current biology : CB
JID - 9107782
SB  - IM
MH  - Humans
MH  - Magnetic Resonance Imaging
MH  - Parietal Lobe/*physiology
MH  - Space Perception/*physiology
EDAT- 2012/12/25 06:00
MHDA- 2013/08/07 06:00
CRDT- 2012/12/25 06:00
PHST- 2012/08/01 [received]
PHST- 2012/10/30 [revised]
PHST- 2012/11/30 [accepted]
PHST- 2012/12/20 [aheadofprint]
AID - S0960-9822(12)01440-6 [pii]
AID - 10.1016/j.cub.2012.11.060 [doi]
PST - ppublish
SO  - Curr Biol. 2013 Jan 21;23(2):177-82. doi: 10.1016/j.cub.2012.11.060. Epub 2012
      Dec 20.

PMID- 23239589
OWN - NLM
STAT- MEDLINE
DA  - 20130912
DCOM- 20140425
LR  - 20141120
IS  - 1478-596X (Electronic)
IS  - 1478-5951 (Linking)
VI  - 9
IP  - 3
DP  - 2013 Sep
TI  - Augmented reality to the rescue of the minimally invasive surgeon. The usefulness
      of the interposition of stereoscopic images in the Da Vinci robotic console.
PG  - e34-8
LID - 10.1002/rcs.1471 [doi]
AB  - BACKGROUND: Computerized management of medical information and 3D imaging has
      become the norm in everyday medical practice. Surgeons exploit these emerging
      technologies and bring information previously confined to the radiology rooms
      into the operating theatre. The paper reports the authors' experience with
      integrated stereoscopic 3D-rendered images in the da Vinci surgeon console.
      METHODS: Volume-rendered images were obtained from a standard computed tomography
      dataset using the OsiriX DICOM workstation. A custom OsiriX plugin was created
      that permitted the 3D-rendered images to be displayed in the da Vinci surgeon
      console and to appear stereoscopic. These rendered images were displayed in the
      robotic console using the TilePro multi-input display. The upper part of the
      screen shows the real endoscopic surgical field and the bottom shows the
      stereoscopic 3D-rendered images. These are controlled by a 3D joystick installed 
      on the console, and are updated in real time. RESULTS: Five patients underwent a 
      robotic augmented reality-enhanced procedure. The surgeon was able to switch
      between the classical endoscopic view and a combined virtual view during the
      procedure. Subjectively, the addition of the rendered images was considered to be
      an undeniable help during the dissection phase. CONCLUSION: With the rapid
      evolution of robotics, computer-aided surgery is receiving increasing interest.
      This paper details the authors' experience with 3D-rendered images projected
      inside the surgical console. The use of this intra-operative mixed reality
      technology is considered very useful by the surgeon. It has been shown that the
      usefulness of this technique is a step toward computer-aided surgery that will
      progress very quickly over the next few years.
CI  - Copyright (c) 2012 John Wiley & Sons, Ltd.
FAU - Volonte, Francesco
AU  - Volonte F
AD  - Clinic for Visceral and Transplantation Surgery, Department of Surgery, Geneva
      University Hospital, Rue Gabrielle-Perret-Gentil 4, 1211, Geneva 14, Switzerland.
FAU - Buchs, Nicolas C
AU  - Buchs NC
FAU - Pugin, Francois
AU  - Pugin F
FAU - Spaltenstein, Joel
AU  - Spaltenstein J
FAU - Schiltz, Boris
AU  - Schiltz B
FAU - Jung, Minoa
AU  - Jung M
FAU - Hagen, Monika
AU  - Hagen M
FAU - Ratib, Osman
AU  - Ratib O
FAU - Morel, Philippe
AU  - Morel P
LA  - eng
PT  - Journal Article
DEP - 20121213
PL  - England
TA  - Int J Med Robot
JT  - The international journal of medical robotics + computer assisted surgery : MRCAS
JID - 101250764
SB  - IM
MH  - Depth Perception
MH  - Humans
MH  - Image Processing, Computer-Assisted
MH  - *Imaging, Three-Dimensional
MH  - Minimally Invasive Surgical Procedures/*instrumentation/statistics & numerical
      data
MH  - Models, Anatomic
MH  - Robotics/*instrumentation/statistics & numerical data
MH  - Surgery, Computer-Assisted/*instrumentation/statistics & numerical data
MH  - Tomography, X-Ray Computed/statistics & numerical data
MH  - User-Computer Interface
OTO - NOTNLM
OT  - augmented reality
OT  - mixed reality
OT  - osiriX
OT  - robotic surgery
EDAT- 2012/12/15 06:00
MHDA- 2014/04/26 06:00
CRDT- 2012/12/15 06:00
PHST- 2012/10/31 [accepted]
PHST- 2012/12/13 [aheadofprint]
AID - 10.1002/rcs.1471 [doi]
PST - ppublish
SO  - Int J Med Robot. 2013 Sep;9(3):e34-8. doi: 10.1002/rcs.1471. Epub 2012 Dec 13.

PMID- 22402687
OWN - NLM
STAT- MEDLINE
DA  - 20120309
DCOM- 20120716
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
VI  - 18
IP  - 4
DP  - 2012 Apr
TI  - Effects of immersion on visual analysis of volume data.
PG  - 597-606
LID - 10.1109/TVCG.2012.42 [doi]
AB  - Volume visualization has been widely used for decades for analyzing datasets
      ranging from 3D medical images to seismic data to paleontological data. Many have
      proposed using immersive virtual reality (VR) systems to view volume
      visualizations, and there is anecdotal evidence of the benefits of VR for this
      purpose. However, there has been very little empirical research exploring the
      effects of higher levels of immersion for volume visualization, and it is not
      known how various components of immersion influence the effectiveness of
      visualization in VR. We conducted a controlled experiment in which we studied the
      independent and combined effects of three components of immersion (head tracking,
      field of regard, and stereoscopic rendering) on the effectiveness of
      visualization tasks with two x-ray microscopic computed tomography datasets. We
      report significant benefits of analyzing volume data in an environment involving 
      those components of immersion. We find that the benefits do not necessarily
      require all three components simultaneously, and that the components have
      variable influence on different task categories. The results of our study improve
      our understanding of the effects of immersion on perceived and actual task
      performance, and provide guidance on the choice of display systems to designers
      seeking to maximize the effectiveness of volume visualization applications.
FAU - Laha, Bireswar
AU  - Laha B
AD  - Center for Human-Computer Interaction and Department of Computer Science,
      Virginia Tech, USA. blaha@vt.edu
FAU - Sensharma, Kriti
AU  - Sensharma K
FAU - Schiffbauer, James D
AU  - Schiffbauer JD
FAU - Bowman, Doug A
AU  - Bowman DA
LA  - eng
GR  - RR025667/RR/NCRR NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
MH  - Adolescent
MH  - Adult
MH  - Animals
MH  - *Computer Graphics
MH  - Databases, Factual/*statistics & numerical data
MH  - Extremities/anatomy & histology/blood supply
MH  - Female
MH  - Fossils
MH  - Humans
MH  - Imaging, Three-Dimensional/statistics & numerical data
MH  - Male
MH  - Mice
MH  - Task Performance and Analysis
MH  - Tissue Scaffolds
MH  - *User-Computer Interface
MH  - X-Ray Microtomography/*statistics & numerical data
MH  - Young Adult
EDAT- 2012/03/10 06:00
MHDA- 2012/07/17 06:00
CRDT- 2012/03/10 06:00
AID - 10.1109/TVCG.2012.42 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2012 Apr;18(4):597-606. doi: 10.1109/TVCG.2012.42.

PMID- 22317040
OWN - NLM
STAT- MEDLINE
DA  - 20120209
DCOM- 20140210
IS  - 1875-9270 (Electronic)
IS  - 1051-9815 (Linking)
VI  - 41 Suppl 1
DP  - 2012
TI  - Adaptive information design for outdoor augmented reality.
PG  - 2187-94
LID - 10.3233/WOR-2012-0441-2187 [doi]
AB  - Augmented Reality focuses on the enrichment of the user's natural field of view
      by consistent integration of text, symbols and interactive three-dimensional
      objects in real time. Placing virtual objects directly into the user's view in a 
      natural context empowers highly dynamic applications. On the other hand, this
      necessitates deliberate choice of information design and density, in particular
      for deployment in hazardous environments like military combat scenarios. As the
      amount of information needed is not foreseeable and strongly depends on the
      individual mission, an appropriate system must offer adequate adaptation
      capabilities. The paper presents a prototypical, vehicle-mountable Augmented
      Reality vision system, designed for enhancing situation awareness in stressful
      urban warfare scenarios. Tracking, as one of the most crucial challenges for
      outdoor Augmented Reality, is accomplished by means of a Differential-GPS
      approach while the type of display to attach can be modified, ranging from ocular
      displays to standard LCD mini-screens. The overall concept also includes
      envisioning of own troops (blue forces), for which a multi-sensor tracking
      approach has been chosen. As a main feature, the system allows switching between 
      different information categories, focusing on friendly, hostile, unidentified or 
      neutral data. Results of an empirical study on the superiority of an in-view
      navigation cue approach conclude the paper.
FAU - Neuhofer, Jan A
AU  - Neuhofer JA
AD  - Fraunhofer Institute for Communication, Information Processing and Ergonomics
      (FKIE), Neuenahrer Str. 20, 53343 Wachtberg, Germany.
      jan.neuhoefer@fkie.fraunhofer.de
FAU - Govaers, Felix
AU  - Govaers F
FAU - El Mokni, Hichem
AU  - El Mokni H
FAU - Alexander, Thomas
AU  - Alexander T
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - Netherlands
TA  - Work
JT  - Work (Reading, Mass.)
JID - 9204382
SB  - IM
MH  - Adult
MH  - *Computer Simulation
MH  - Computer Systems
MH  - Empirical Research
MH  - Germany
MH  - Humans
MH  - Male
MH  - Military Personnel
MH  - Military Science/*methods
MH  - *Software Design
MH  - *User-Computer Interface
MH  - War
MH  - Young Adult
EDAT- 2012/02/10 06:00
MHDA- 2014/02/11 06:00
CRDT- 2012/02/10 06:00
AID - BQ0632Q474310576 [pii]
AID - 10.3233/WOR-2012-0441-2187 [doi]
PST - ppublish
SO  - Work. 2012;41 Suppl 1:2187-94. doi: 10.3233/WOR-2012-0441-2187.

PMID- 22105016
OWN - NLM
STAT- MEDLINE
DA  - 20120727
DCOM- 20121001
IS  - 1941-0506 (Electronic)
IS  - 1077-2626 (Linking)
VI  - 18
IP  - 7
DP  - 2012 Jul
TI  - Interactive visibility retargeting in VR using conformal visualization.
PG  - 1027-40
LID - 10.1109/TVCG.2011.278 [doi]
AB  - In Virtual Reality, immersive systems such as the CAVE provide an important tool 
      for the collaborative exploration of large 3D data. Unlike head-mounted displays,
      these systems are often only partially immersive due to space, access, or cost
      constraints. The resulting loss of visual information becomes a major obstacle
      for critical tasks that need to utilize the users' entire field of vision. We
      have developed a conformal visualization technique that establishes a conformal
      mapping between the full 360 degrees field of view and the display geometry of a 
      given visualization system. The mapping is provably angle-preserving and has the 
      desirable property of preserving shapes locally, which is important for
      identifying shape-based features in the visual data. We apply the conformal
      visualization to both forward and backward rendering pipelines in a variety of
      retargeting scenarios, including CAVEs and angled arrangements of flat panel
      displays. In contrast to image-based retargeting approaches, our technique
      constructs accurate stereoscopic images that are free of resampling artifacts.
      Our user study shows that on the visual polyp detection task in Immersive Virtual
      Colonoscopy, conformal visualization leads to improved sensitivity at comparable 
      examination times against the traditional rendering approach. We also develop a
      novel user interface based on the interactive recreation of the conformal mapping
      and the real-time regeneration of the view direction correspondence.
FAU - Petkov, Kaloian
AU  - Petkov K
AD  - Department of Computer Science, Stony Brook University, Stony Brook, NY 11794,
      USA. kpetkov@cs.stonybrook.edu
FAU - Papadopoulos, Charilaos
AU  - Papadopoulos C
FAU - Zhang, Min
AU  - Zhang M
FAU - Kaufman, Arie E
AU  - Kaufman AE
FAU - Gu, Xianfeng David
AU  - Gu XD
LA  - eng
GR  - R01EB7530/EB/NIBIB NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
MH  - Algorithms
MH  - Colonic Polyps/diagnosis
MH  - Colonoscopy/methods
MH  - *Computer Graphics
MH  - Depth Perception/*physiology
MH  - Humans
MH  - *Models, Theoretical
MH  - Sensitivity and Specificity
MH  - *User-Computer Interface
EDAT- 2011/11/23 06:00
MHDA- 2012/10/02 06:00
CRDT- 2011/11/23 06:00
AID - 10.1109/TVCG.2011.278 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2012 Jul;18(7):1027-40. doi: 10.1109/TVCG.2011.278.

PMID- 21802281
OWN - NLM
STAT- MEDLINE
DA  - 20110816
DCOM- 20111208
LR  - 20141120
IS  - 1879-3320 (Electronic)
IS  - 0960-7404 (Linking)
VI  - 20
IP  - 3
DP  - 2011 Sep
TI  - Augmented reality in laparoscopic surgical oncology.
PG  - 189-201
LID - 10.1016/j.suronc.2011.07.002 [doi]
AB  - Minimally invasive surgery represents one of the main evolutions of surgical
      techniques aimed at providing a greater benefit to the patient. However,
      minimally invasive surgery increases the operative difficulty since the depth
      perception is usually dramatically reduced, the field of view is limited and the 
      sense of touch is transmitted by an instrument. However, these drawbacks can
      currently be reduced by computer technology guiding the surgical gesture. Indeed,
      from a patient's medical image (US, CT or MRI), Augmented Reality (AR) can
      increase the surgeon's intra-operative vision by providing a virtual transparency
      of the patient. AR is based on two main processes: the 3D visualization of the
      anatomical or pathological structures appearing in the medical image, and the
      registration of this visualization on the real patient. 3D visualization can be
      performed directly from the medical image without the need for a pre-processing
      step thanks to volume rendering. But better results are obtained with surface
      rendering after organ and pathology delineations and 3D modelling. Registration
      can be performed interactively or automatically. Several interactive systems have
      been developed and applied to humans, demonstrating the benefit of AR in surgical
      oncology. It also shows the current limited interactivity due to soft organ
      movements and interaction between surgeon instruments and organs. If the current 
      automatic AR systems show the feasibility of such system, it is still relying on 
      specific and expensive equipment which is not available in clinical routine.
      Moreover, they are not robust enough due to the high complexity of developing a
      real-time registration taking organ deformation and human movement into account. 
      However, the latest results of automatic AR systems are extremely encouraging and
      show that it will become a standard requirement for future computer-assisted
      surgical oncology. In this article, we will explain the concept of AR and its
      principles. Then, we will review the existing interactive and automatic AR
      systems in digestive surgical oncology, highlighting their benefits and
      limitations. Finally, we will discuss the future evolutions and the issues that
      still have to be tackled so that this technology can be seamlessly integrated in 
      the operating room.
CI  - Copyright (c) 2011 Elsevier Ltd. All rights reserved.
FAU - Nicolau, Stephane
AU  - Nicolau S
AD  - IRCAD/EITS, Hopitaux Universitaires de Strasbourg, Digestive and Endocrine
      Surgery, 1 Place de l'Hopital, 67091 Strasbourg Cedex, France.
FAU - Soler, Luc
AU  - Soler L
FAU - Mutter, Didier
AU  - Mutter D
FAU - Marescaux, Jacques
AU  - Marescaux J
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20110728
PL  - Netherlands
TA  - Surg Oncol
JT  - Surgical oncology
JID - 9208188
SB  - IM
MH  - Computer Simulation
MH  - *Diagnostic Imaging
MH  - *General Surgery
MH  - Humans
MH  - Image Processing, Computer-Assisted
MH  - *Laparoscopy
MH  - *Medical Oncology
MH  - *Minimally Invasive Surgical Procedures
MH  - Neoplasms/pathology/*surgery
MH  - Software
EDAT- 2011/08/02 06:00
MHDA- 2011/12/13 00:00
CRDT- 2011/08/02 06:00
PHST- 2011/07/28 [aheadofprint]
AID - S0960-7404(11)00052-1 [pii]
AID - 10.1016/j.suronc.2011.07.002 [doi]
PST - ppublish
SO  - Surg Oncol. 2011 Sep;20(3):189-201. doi: 10.1016/j.suronc.2011.07.002. Epub 2011 
      Jul 28.

PMID- 21684505
OWN - NLM
STAT- MEDLINE
DA  - 20110919
DCOM- 20120127
LR  - 20131121
IS  - 1873-6297 (Electronic)
IS  - 0001-6918 (Linking)
VI  - 138
IP  - 1
DP  - 2011 Sep
TI  - Comparison of grasping movements made by healthy subjects in a 3-dimensional
      immersive virtual versus physical environment.
PG  - 126-34
LID - 10.1016/j.actpsy.2011.05.015 [doi]
AB  - Virtual reality (VR) technology is being used with increasing frequency as a
      training medium for motor rehabilitation. However, before addressing training
      effectiveness in virtual environments (VEs), it is necessary to identify if
      movements made in such environments are kinematically similar to those made in
      physical environments (PEs) and the effect of provision of haptic feedback on
      these movement patterns. These questions are important since reach-to-grasp
      movements may be inaccurate when visual or haptic feedback is altered or absent. 
      Our goal was to compare kinematics of reaching and grasping movements to three
      objects performed in an immersive three-dimensional (3D) VE with haptic feedback 
      (cyberglove/grasp system) viewed through a head-mounted display to those made in 
      an equivalent physical environment (PE). We also compared movements in PE made
      with and without wearing the cyberglove/grasp haptic feedback system. Ten healthy
      subjects (8 women, 62.1+/-8.8years) reached and grasped objects requiring 3
      different grasp types (can, diameter 65.6mm, cylindrical grasp; screwdriver,
      diameter 31.6mm, power grasp; pen, diameter 7.5mm, precision grasp) in PE and
      visually similar virtual objects in VE. Temporal and spatial arm and trunk
      kinematics were analyzed. Movements were slower and grip apertures were wider
      when wearing the glove in both the PE and the VE compared to movements made in
      the PE without the glove. When wearing the glove, subjects used similar reaching 
      trajectories in both environments, preserved the coordination between reaching
      and grasping and scaled grip aperture to object size for the larger object
      (cylindrical grasp). However, in VE compared to PE, movements were slower and had
      longer deceleration times, elbow extension was greater when reaching to the
      smallest object and apertures were wider for the power and precision grip tasks. 
      Overall, the differences in spatial and temporal kinematics of movements between 
      environments were greater than those due only to wearing the cyberglove/grasp
      system. Differences in movement kinematics due to the viewing environment were
      likely due to a lack of prior experience with the virtual environment, an
      uncertainty of object location and the restricted field-of-view when wearing the 
      head-mounted display. The results can be used to inform the design and
      disposition of objects within 3D VEs for the study of the control of prehension
      and for upper limb rehabilitation.
CI  - Copyright (c) 2011 Elsevier B.V. All rights reserved.
FAU - Magdalon, Eliane C
AU  - Magdalon EC
AD  - Department of Biomedical Engineering, University of Campinas (UNICAMP), School of
      Electrical and Computer Engineering (FEEC), Campinas, SP, Brazil.
FAU - Michaelsen, Stella M
AU  - Michaelsen SM
FAU - Quevedo, Antonio A
AU  - Quevedo AA
FAU - Levin, Mindy F
AU  - Levin MF
LA  - eng
PT  - Comparative Study
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - Netherlands
TA  - Acta Psychol (Amst)
JT  - Acta psychologica
JID - 0370366
SB  - IM
MH  - Aged
MH  - Biomechanical Phenomena/physiology
MH  - *Environment
MH  - Female
MH  - Hand Strength/*physiology
MH  - Humans
MH  - Male
MH  - Middle Aged
MH  - Orientation/physiology
MH  - Psychomotor Performance/*physiology
MH  - *User-Computer Interface
EDAT- 2011/06/21 06:00
MHDA- 2012/01/28 06:00
CRDT- 2011/06/21 06:00
PHST- 2010/08/13 [received]
PHST- 2011/05/21 [revised]
PHST- 2011/05/23 [accepted]
AID - S0001-6918(11)00109-0 [pii]
AID - 10.1016/j.actpsy.2011.05.015 [doi]
PST - ppublish
SO  - Acta Psychol (Amst). 2011 Sep;138(1):126-34. doi: 10.1016/j.actpsy.2011.05.015.

PMID- 21668293
OWN - NLM
STAT- MEDLINE
DA  - 20110616
DCOM- 20111024
LR  - 20140730
IS  - 1097-0150 (Electronic)
IS  - 1092-9088 (Linking)
VI  - 16
IP  - 4
DP  - 2011
TI  - Fusion and visualization of intraoperative cortical images with preoperative
      models for epilepsy surgical planning and guidance.
PG  - 149-60
LID - 10.3109/10929088.2011.585805 [doi]
AB  - OBJECTIVE: During epilepsy surgery it is important for the surgeon to correlate
      the preoperative cortical morphology (from preoperative images) with the
      intraoperative environment. Augmented Reality (AR) provides a solution for
      combining the real environment with virtual models. However, AR usually requires 
      the use of specialized displays, and its effectiveness in the surgery still needs
      to be evaluated. The objective of this research was to develop an alternative
      approach to provide enhanced visualization by fusing a direct (photographic) view
      of the surgical field with the 3D patient model during image guided epilepsy
      surgery. MATERIALS AND METHODS: We correlated the preoperative plan with the
      intraoperative surgical scene, first by a manual landmark-based registration and 
      then by an intensity-based perspective 3D-2D registration for camera pose
      estimation. The 2D photographic image was then texture-mapped onto the 3D
      preoperative model using the solved camera pose. In the proposed method, we
      employ direct volume rendering to obtain a perspective view of the brain image
      using GPU-accelerated ray-casting. The algorithm was validated by a phantom study
      and also in the clinical environment with a neuronavigation system. RESULTS: In
      the phantom experiment, the 3D Mean Registration Error (MRE) was 2.43 +/- 0.32 mm
      with a success rate of 100%. In the clinical experiment, the 3D MRE was 5.15 +/- 
      0.49 mm with 2D in-plane error of 3.30 +/- 1.41 mm. A clinical application of our
      fusion method for enhanced and augmented visualization for integrated image and
      functional guidance during neurosurgery is also presented. CONCLUSIONS: This
      paper presents an alternative approach to a sophisticated AR environment for
      assisting in epilepsy surgery, whereby a real intraoperative scene is mapped onto
      the surface model of the brain. In contrast to the AR approach, this method needs
      no specialized display equipment. Moreover, it requires minimal changes to
      existing systems and workflow, and is therefore well suited to the OR
      environment. In the phantom and in vivo clinical experiments, we demonstrate that
      the fusion method can achieve a level of accuracy sufficient for the requirements
      of epilepsy surgery.
FAU - Wang, A
AU  - Wang A
AD  - Imaging Research Laboratories, Robarts Research Institute , London, Ontario.
FAU - Mirsattari, S M
AU  - Mirsattari SM
FAU - Parrent, A G
AU  - Parrent AG
FAU - Peters, T M
AU  - Peters TM
LA  - eng
GR  - MOP 184807/Canadian Institutes of Health Research/Canada
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20110613
PL  - England
TA  - Comput Aided Surg
JT  - Computer aided surgery : official journal of the International Society for
      Computer Aided Surgery
JID - 9708375
SB  - IM
MH  - Cerebral Cortex/*surgery
MH  - Craniotomy
MH  - Electroencephalography
MH  - Epilepsy, Temporal Lobe/*surgery
MH  - Humans
MH  - Image Processing, Computer-Assisted
MH  - Magnetic Resonance Imaging
MH  - Neuronavigation/*methods
MH  - Phantoms, Imaging
EDAT- 2011/06/15 06:00
MHDA- 2011/10/25 06:00
CRDT- 2011/06/15 06:00
PHST- 2011/06/13 [aheadofprint]
AID - 10.3109/10929088.2011.585805 [doi]
PST - ppublish
SO  - Comput Aided Surg. 2011;16(4):149-60. doi: 10.3109/10929088.2011.585805. Epub
      2011 Jun 13.

PMID- 21414339
OWN - NLM
STAT- MEDLINE
DA  - 20110509
DCOM- 20110812
LR  - 20150206
IS  - 1878-5646 (Electronic)
IS  - 0042-6989 (Linking)
VI  - 51
IP  - 10
DP  - 2011 May 25
TI  - Differential impact of partial cortical blindness on gaze strategies when sitting
      and walking - an immersive virtual reality study.
PG  - 1173-84
LID - 10.1016/j.visres.2011.03.006 [doi]
AB  - The present experiments aimed to characterize the visual performance of subjects 
      with long-standing, unilateral cortical blindness when walking in a naturalistic,
      virtual environment. Under static, seated testing conditions, cortically blind
      subjects are known to exhibit compensatory eye movement strategies. However, they
      still complain of significant impairment in visual detection during navigation.
      To assess whether this is due to a change in compensatory eye movement strategy
      between sitting and walking, we measured eye and head movements in subjects asked
      to detect peripherally-presented, moving basketballs. When seated, cortically
      blind subjects detected approximately 80% of balls, while controls detected
      almost all balls. Seated blind subjects did not make larger head movements than
      controls, but they consistently biased their fixation distribution towards their 
      blind hemifield. When walking, head movements were similar in the two groups, but
      the fixation bias decreased to the point that fixation distribution in cortically
      blind subjects became similar to that in controls - with one major exception: at 
      the time of basketball appearance, walking controls looked primarily at the far
      ground, in upper quadrants of the virtual field of view; cortically blind
      subjects looked significantly more at the near ground, in lower quadrants of the 
      virtual field. Cortically blind subjects detected only 58% of the balls when
      walking while controls detected approximately 90%. Thus, the adaptive gaze
      strategies adopted by cortically blind individuals as a compensation for their
      visual loss are strongest and most effective when seated and stationary. Walking 
      significantly alters these gaze strategies in a way that seems to favor walking
      performance, but impairs peripheral target detection. It is possible that this
      impairment underlies the experienced difficulty of those with cortical blindness 
      when navigating in real life.
CI  - Copyright (c) 2011 Elsevier Ltd. All rights reserved.
FAU - Iorizzo, Dana B
AU  - Iorizzo DB
AD  - Flaum Eye Institute, University of Rochester, 601 Elmwood Ave., Box 314,
      Rochester, NY 14642, USA. dana_iorizzo@urmc.rochester.edu
FAU - Riley, Meghan E
AU  - Riley ME
FAU - Hayhoe, Mary
AU  - Hayhoe M
FAU - Huxlin, Krystel R
AU  - Huxlin KR
LA  - eng
GR  - P0EY01319F/EY/NEI NIH HHS/United States
GR  - P30 EY001319/EY/NEI NIH HHS/United States
GR  - P30 EY001319-38/EY/NEI NIH HHS/United States
GR  - R01 EY005729/EY/NEI NIH HHS/United States
GR  - R01 EY005729-25A2/EY/NEI NIH HHS/United States
GR  - R01 EY005729-26/EY/NEI NIH HHS/United States
GR  - R01 EY021209/EY/NEI NIH HHS/United States
GR  - R01 EY05729/EY/NEI NIH HHS/United States
GR  - RR09283/RR/NCRR NIH HHS/United States
GR  - T-32 NS007489/NS/NINDS NIH HHS/United States
GR  - T32 EY007125/EY/NEI NIH HHS/United States
GR  - T32 EY007125-22/EY/NEI NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
DEP - 20110322
PL  - England
TA  - Vision Res
JT  - Vision research
JID - 0417402
SB  - IM
MH  - Aged
MH  - Aged, 80 and over
MH  - Blindness, Cortical/*physiopathology
MH  - Eye Movements/physiology
MH  - Female
MH  - Fixation, Ocular/*physiology
MH  - Head Movements/physiology
MH  - Humans
MH  - Male
MH  - Middle Aged
MH  - Motion Perception
MH  - *Posture
MH  - User-Computer Interface
MH  - Visual Perception/*physiology
MH  - *Walking
PMC - PMC3093191
MID - NIHMS282244
OID - NLM: NIHMS282244
OID - NLM: PMC3093191
EDAT- 2011/03/19 06:00
MHDA- 2011/08/13 06:00
CRDT- 2011/03/19 06:00
PHST- 2010/07/21 [received]
PHST- 2011/01/12 [revised]
PHST- 2011/03/10 [accepted]
PHST- 2011/03/22 [aheadofprint]
AID - S0042-6989(11)00101-5 [pii]
AID - 10.1016/j.visres.2011.03.006 [doi]
PST - ppublish
SO  - Vision Res. 2011 May 25;51(10):1173-84. doi: 10.1016/j.visres.2011.03.006. Epub
      2011 Mar 22.

PMID- 21283823
OWN - NLM
STAT- MEDLINE
DA  - 20110201
DCOM- 20110802
LR  - 20140821
IS  - 1932-6203 (Electronic)
IS  - 1932-6203 (Linking)
VI  - 6
IP  - 1
DP  - 2011
TI  - Multisensory stimulation can induce an illusion of larger belly size in immersive
      virtual reality.
PG  - e16128
LID - 10.1371/journal.pone.0016128 [doi]
AB  - BACKGROUND: Body change illusions have been of great interest in recent years for
      the understanding of how the brain represents the body. Appropriate multisensory 
      stimulation can induce an illusion of ownership over a rubber or virtual arm,
      simple types of out-of-the-body experiences, and even ownership with respect to
      an alternate whole body. Here we use immersive virtual reality to investigate
      whether the illusion of a dramatic increase in belly size can be induced in males
      through (a) first person perspective position (b) synchronous visual-motor
      correlation between real and virtual arm movements, and (c) self-induced
      synchronous visual-tactile stimulation in the stomach area. METHODOLOGY: Twenty
      two participants entered into a virtual reality (VR) delivered through a stereo
      head-tracked wide field-of-view head-mounted display. They saw from a first
      person perspective a virtual body substituting their own that had an inflated
      belly. For four minutes they repeatedly prodded their real belly with a rod that 
      had a virtual counterpart that they saw in the VR. There was a synchronous
      condition where their prodding movements were synchronous with what they felt and
      saw and an asynchronous condition where this was not the case. The experiment was
      repeated twice for each participant in counter-balanced order. Responses were
      measured by questionnaire, and also a comparison of before and after
      self-estimates of belly size produced by direct visual manipulation of the
      virtual body seen from the first person perspective. CONCLUSIONS: The results
      show that first person perspective of a virtual body that substitutes for the own
      body in virtual reality, together with synchronous multisensory stimulation can
      temporarily produce changes in body representation towards the larger belly size.
      This was demonstrated by (a) questionnaire results, (b) the difference between
      the self-estimated belly size, judged from a first person perspective, after and 
      before the experimental manipulation, and (c) significant positive correlations
      between these two measures. We discuss this result in the general context of body
      ownership illusions, and suggest applications including treatment for body size
      distortion illnesses.
FAU - Normand, Jean-Marie
AU  - Normand JM
AD  - EVENT Lab, Facultat de Psicologia, Universitat de Barcelona, Barcelona, Spain.
FAU - Giannopoulos, Elias
AU  - Giannopoulos E
FAU - Spanlang, Bernhard
AU  - Spanlang B
FAU - Slater, Mel
AU  - Slater M
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
DEP - 20110119
PL  - United States
TA  - PLoS One
JT  - PloS one
JID - 101285081
SB  - IM
MH  - *Body Image
MH  - Humans
MH  - Illusions/*etiology
MH  - Male
MH  - *Photic Stimulation
MH  - Questionnaires
MH  - Stomach
MH  - *Touch Perception
MH  - *User-Computer Interface
PMC - PMC3023777
OID - NLM: PMC3023777
EDAT- 2011/02/02 06:00
MHDA- 2011/08/04 06:00
CRDT- 2011/02/02 06:00
PHST- 2010/08/06 [received]
PHST- 2010/12/13 [accepted]
PHST- 2011/01/19 [epublish]
AID - 10.1371/journal.pone.0016128 [doi]
PST - epublish
SO  - PLoS One. 2011 Jan 19;6(1):e16128. doi: 10.1371/journal.pone.0016128.

PMID- 26146424
OWN - NLM
STAT- Publisher
DA  - 20150706
LR  - 20150719
IS  - 0097-966X (Print)
VI  - 41
IP  - 1
DP  - 2010 May 1
TI  - 64.1: Display Technologies for Therapeutic Applications of Virtual Reality.
PG  - 949-952
AB  - A paradigm shift in image source technology for VR helmets is needed. Using
      scanning fiber displays to replace LCD displays creates lightweight, safe, low
      cost, wide field of view, portable VR goggles ideal for reducing pain during
      severe burn wound care in hospitals and possibly in austere combat-transport
      environments.
FAU - Hoffman, Hunter G
AU  - Hoffman HG
AD  - Human Photonics Laboratory and Human Interface Technology Lab Dept. of Mechanical
      Engineering, University of Washington, Seattle, WA, USA.
FAU - Schowengerdt, Brian T
AU  - Schowengerdt BT
AD  - Human Photonics Laboratory and Human Interface Technology Lab Dept. of Mechanical
      Engineering, University of Washington, Seattle, WA, USA.
FAU - Lee, Cameron M
AU  - Lee CM
AD  - Human Photonics Laboratory and Human Interface Technology Lab Dept. of Mechanical
      Engineering, University of Washington, Seattle, WA, USA.
FAU - Magula, Jeff
AU  - Magula J
AD  - Human Photonics Laboratory and Human Interface Technology Lab Dept. of Mechanical
      Engineering, University of Washington, Seattle, WA, USA.
FAU - Seibel, Eric J
AU  - Seibel EJ
AD  - Human Photonics Laboratory and Human Interface Technology Lab Dept. of Mechanical
      Engineering, University of Washington, Seattle, WA, USA.
LA  - ENG
GR  - R01 AR054115/AR/NIAMS NIH HHS/United States
GR  - R01 GM042725/GM/NIGMS NIH HHS/United States
PT  - JOURNAL ARTICLE
TA  - Dig Tech Pap
JT  - Digest of technical papers. SID International Symposium
JID - 101660608
PMC - PMC4487527
MID - NIHMS696940
EDAT- 2010/05/01 00:00
MHDA- 2010/05/01 00:00
CRDT- 2015/07/07 06:00
AID - 10.1889/1.3500639 [doi]
PST - ppublish
SO  - Dig Tech Pap. 2010 May 1;41(1):949-952.

PMID- 22275200
OWN - NLM
STAT- MEDLINE
DA  - 20120125
DCOM- 20120820
LR  - 20141120
IS  - 1941-1189 (Electronic)
IS  - 1937-3333 (Linking)
VI  - 3
DP  - 2010
TI  - Virtual and augmented medical imaging environments: enabling technology for
      minimally invasive cardiac interventional guidance.
PG  - 25-47
LID - 10.1109/RBME.2010.2082522 [doi]
AB  - Virtual and augmented reality environments have been adopted in medicine as a
      means to enhance the clinician's view of the anatomy and facilitate the
      performance of minimally invasive procedures. Their value is truly appreciated
      during interventions where the surgeon cannot directly visualize the targets to
      be treated, such as during cardiac procedures performed on the beating heart.
      These environments must accurately represent the real surgical field and require 
      seamless integration of pre- and intra-operative imaging, surgical tracking, and 
      visualization technology in a common framework centered around the patient. This 
      review begins with an overview of minimally invasive cardiac interventions,
      describes the architecture of a typical surgical guidance platform including
      imaging, tracking, registration and visualization, highlights both clinical and
      engineering accuracy limitations in cardiac image guidance, and discusses the
      translation of the work from the laboratory into the operating room together with
      typically encountered challenges.
FAU - Linte, Cristian A
AU  - Linte CA
AD  - Biomedical Engineering Graduate Program, University of Western Ontario, London.
      clinte@imaging.robarts.ca
FAU - White, James
AU  - White J
FAU - Eagleson, Roy
AU  - Eagleson R
FAU - Guiraudon, Gerard M
AU  - Guiraudon GM
FAU - Peters, Terry M
AU  - Peters TM
LA  - eng
GR  - Canadian Institutes of Health Research/Canada
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Review
PL  - United States
TA  - IEEE Rev Biomed Eng
JT  - IEEE reviews in biomedical engineering
JID - 101493803
SB  - IM
MH  - Cardiac Surgical Procedures/*methods
MH  - Diagnostic Imaging/*methods
MH  - Humans
MH  - Magnetic Resonance Imaging
MH  - Minimally Invasive Surgical Procedures/instrumentation/*methods
EDAT- 2010/01/01 00:00
MHDA- 2012/08/21 06:00
CRDT- 2012/01/26 06:00
AID - 10.1109/RBME.2010.2082522 [doi]
PST - ppublish
SO  - IEEE Rev Biomed Eng. 2010;3:25-47. doi: 10.1109/RBME.2010.2082522.

PMID- 19963991
OWN - NLM
STAT- MEDLINE
DA  - 20091207
DCOM- 20100422
LR  - 20140821
IS  - 1557-170X (Print)
IS  - 1557-170X (Linking)
VI  - 2009
DP  - 2009
TI  - Linear vection in virtual environments can be strengthened by discordant inertial
      input.
PG  - 1157-60
LID - 10.1109/IEMBS.2009.5333425 [doi]
AB  - Visual and gravitoinertial sensory inputs are integrated by the central nervous
      system to provide a compelling and veridical sense of spatial orientation and
      motion. Although it's known that visual input alone can drive this perception,
      questions remain as to how vestibular/ proprioceptive (i.e. inertial) inputs
      integrate with visual input to affect this process. This was investigated further
      by combining sinusoidal vertical linear oscillation (5 amplitudes between 0m and 
      +/-0.8m) with two different virtual visual inputs. Visual scenes were viewed in a
      large field-of-view head-mounted display (HMD), which depicted an enriched,
      hi-res, dynamic image of the actual test chamber from the perspective of a
      subject seated in the linear motion device. The scene either depicted horizontal 
      (+/-0.7m) or vertical (+/-0.8m) linear 0.2Hz sinusoidal translation. Horizontal
      visual motion with vertical inertial motion represents a 90 degrees spatial
      shift. Vertical visual motion with vertical inertial motion whereby the highest
      physical point matches the lowest visual point and vice versa represents a 180
      degrees temporal shift, i.e. opposite of what one experiences in reality.
      Inertial-only stimulation without visual input was identified as vertical linear 
      oscillation with accurate reports of acceleration peaks and troughs, but a slight
      tendency to underestimate amplitude. Visual-only (stationary) stimulation was
      less compelling than combined visual+inertial conditions. In visual+inertial
      conditions, visual input dominated the direction of perceived self-motion,
      however, increasing the inertial amplitude increased how compelling this
      non-veridical perception was. That is, perceived vertical self-motion was most
      compelling when inertial stimulation was maximal, despite perceiving "up" when
      physically "down" and vice versa. Similarly, perceived horizontal self-motion was
      most compelling when vertical inertial motion was at maximum amplitude.
      "Cross-talk" between visual and vestibular channels was suggested by reports of
      small vertical components of perceived self-motion combined with a dominant
      horizontal component. In conclusion, direction of perceived self-motion was
      dominated by visual motion, however, compellingness of this illusion was
      strengthened by increasing discordant inertial input. Thus, spatial mapping of
      inertial systems may be completely labile, while amplitude coding of the input
      intensifies the percept.
FAU - Wright, W Geoffrey
AU  - Wright WG
AD  - Brandeis University, Waltham, MA 02454, USA. wrightw@temple.edu
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Conf Proc IEEE Eng Med Biol Soc
JT  - Conference proceedings : ... Annual International Conference of the IEEE
      Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and
      Biology Society. Annual Conference
JID - 101243413
SB  - IM
MH  - Adolescent
MH  - Humans
MH  - Linear Models
MH  - Models, Neurological
MH  - Motion Perception/*physiology
MH  - Orientation/physiology
MH  - Psychomotor Performance/physiology
MH  - Spatial Behavior/physiology
MH  - *User-Computer Interface
MH  - Vestibule, Labyrinth/physiology
MH  - Visual Perception/physiology
MH  - Young Adult
EDAT- 2009/12/08 06:00
MHDA- 2010/04/23 06:00
CRDT- 2009/12/08 06:00
AID - 10.1109/IEMBS.2009.5333425 [doi]
PST - ppublish
SO  - Conf Proc IEEE Eng Med Biol Soc. 2009;2009:1157-60. doi:
      10.1109/IEMBS.2009.5333425.

PMID- 19592796
OWN - NLM
STAT- MEDLINE
DA  - 20090713
DCOM- 20090929
LR  - 20140828
IS  - 0926-9630 (Print)
IS  - 0926-9630 (Linking)
VI  - 145
DP  - 2009
TI  - Postural and spatial orientation driven by virtual reality.
PG  - 209-28
AB  - Orientation in space is a perceptual variable intimately related to postural
      orientation that relies on visual and vestibular signals to correctly identify
      our position relative to vertical. We have combined a virtual environment with
      motion of a posture platform to produce visual-vestibular conditions that allow
      us to explore how motion of the visual environment may affect perception of
      vertical and, consequently, affect postural stabilizing responses. In order to
      involve a higher level perceptual process, we needed to create a visual
      environment that was immersive. We did this by developing visual scenes that
      possess contextual information using color, texture, and 3-dimensional
      structures. Update latency of the visual scene was close to physiological
      latencies of the vestibulo-ocular reflex. Using this system we found that even
      when healthy young adults stand and walk on a stable support surface, they are
      unable to ignore wide field of view visual motion and they adapt their postural
      orientation to the parameters of the visual motion. Balance training within our
      environment elicited measurable rehabilitation outcomes. Thus we believe that
      virtual environments can serve as a clinical tool for evaluation and training of 
      movement in situations that closely reflect conditions found in the physical
      world.
FAU - Keshner, Emily A
AU  - Keshner EA
AD  - Department of Physical Therapy, College of Health Professions and Department of
      Electrical and Computer Engineering, College of Engineering, Temple University,
      Philadelphia, USA.
FAU - Kenyon, Robert V
AU  - Kenyon RV
LA  - eng
GR  - AG16359/AG/NIA NIH HHS/United States
GR  - AG26470/AG/NIA NIH HHS/United States
GR  - DC01125/DC/NIDCD NIH HHS/United States
GR  - DC05235/DC/NIDCD NIH HHS/United States
GR  - R01 AG026470/AG/NIA NIH HHS/United States
GR  - R01 AG026470-03/AG/NIA NIH HHS/United States
PT  - Journal Article
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - Netherlands
TA  - Stud Health Technol Inform
JT  - Studies in health technology and informatics
JID - 9214582
SB  - T
MH  - *Computer Simulation
MH  - Humans
MH  - Orientation/*physiology
MH  - *Posture
MH  - *User-Computer Interface
PMC - PMC2736108
MID - NIHMS136934
OID - NLM: NIHMS136934
OID - NLM: PMC2736108
EDAT- 2009/07/14 09:00
MHDA- 2009/09/30 06:00
CRDT- 2009/07/14 09:00
PST - ppublish
SO  - Stud Health Technol Inform. 2009;145:209-28.

PMID- 19574833
OWN - NLM
STAT- MEDLINE
DA  - 20090703
DCOM- 20090915
IS  - 1524-4040 (Electronic)
IS  - 0148-396X (Linking)
VI  - 65
IP  - 1
DP  - 2009 Jul
TI  - Prediction of surgical view of neurovascular decompression using interactive
      computer graphics.
PG  - 121-8; discussion 128-9
LID - 10.1227/01.NEU.0000347890.19718.0A [doi]
AB  - OBJECTIVE: To assess the value of an interactive visualization method for
      detecting the offending vessels in neurovascular compression syndrome in patients
      with facial spasm and trigeminal neuralgia. Computer graphics models are created 
      by fusion of fast imaging employing steady-state acquisition and magnetic
      resonance angiography. METHODS: High-resolution magnetic resonance angiography
      and fast imaging employing steady-state acquisition were performed preoperatively
      in 17 patients with neurovascular compression syndromes (facial spasm, n = 10;
      trigeminal neuralgia, n = 7) using a 3.0-T magnetic resonance imaging scanner.
      Computer graphics models were created with computer software and observed
      interactively for detection of offending vessels by rotation, enlargement,
      reduction, and retraction on a graphic workstation. Two-dimensional images were
      reviewed by 2 radiologists blinded to the clinical details, and 2 neurosurgeons
      predicted the offending vessel with the interactive visualization method before
      surgery. Predictions from the 2 imaging approaches were compared with surgical
      findings. The vessels identified during surgery were assumed to be the true
      offending vessels. RESULTS: Offending vessels were identified correctly in 16 of 
      17 patients (94%) using the interactive visualization method and in 10 of 17
      patients using 2-dimensional images. These data demonstrated a significant
      difference (P = 0.015 by Fisher's exact method). CONCLUSION: The interactive
      visualization method data corresponded well with surgical findings (surgical
      field, offending vessels, and nerves). Virtual reality 3-dimensional computer
      graphics using fusion magnetic resonance angiography and fast imaging employing
      steady-state acquisition may be helpful for preoperative simulation.
FAU - Kin, Taichi
AU  - Kin T
AD  - Department of Neurosurgery, University of Tokyo, Tokyo, Japan.
      tkin-tky@umin.ac.jp
FAU - Oyama, Hiroshi
AU  - Oyama H
FAU - Kamada, Kyousuke
AU  - Kamada K
FAU - Aoki, Shigeki
AU  - Aoki S
FAU - Ohtomo, Kuni
AU  - Ohtomo K
FAU - Saito, Nobuhito
AU  - Saito N
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Neurosurgery
JT  - Neurosurgery
JID - 7802914
SB  - IM
MH  - Adult
MH  - Aged
MH  - Decompression, Surgical/methods
MH  - Female
MH  - Hemifacial Spasm/*radiography/surgery
MH  - Humans
MH  - Image Processing, Computer-Assisted/*methods
MH  - Magnetic Resonance Angiography/*methods
MH  - Magnetic Resonance Imaging/methods
MH  - Male
MH  - Microcirculation
MH  - Middle Aged
MH  - Predictive Value of Tests
MH  - Preoperative Care/*methods
MH  - Retrospective Studies
MH  - Surgery, Computer-Assisted/methods
MH  - Trigeminal Neuralgia/*radiography/surgery
EDAT- 2009/07/04 09:00
MHDA- 2009/09/16 06:00
CRDT- 2009/07/04 09:00
AID - 10.1227/01.NEU.0000347890.19718.0A [doi]
AID - 00006123-200907000-00024 [pii]
PST - ppublish
SO  - Neurosurgery. 2009 Jul;65(1):121-8; discussion 128-9. doi:
      10.1227/01.NEU.0000347890.19718.0A.

PMID- 19151879
OWN - NLM
STAT- MEDLINE
DA  - 20090119
DCOM- 20090305
IS  - 0026-1270 (Print)
IS  - 0026-1270 (Linking)
VI  - 48
IP  - 1
DP  - 2009
TI  - Medical image computing for computer-supported diagnostics and therapy. Advances 
      and perspectives.
PG  - 11-7
AB  - OBJECTIVES: Medical image computing has become one of the most challenging fields
      in medical informatics. In image-based diagnostics of the future software
      assistance will become more and more important, and image analysis systems
      integrating advanced image computing methods are needed to extract quantitative
      image parameters to characterize the state and changes of image structures of
      interest (e.g. tumors, organs, vessels, bones etc.) in a reproducible and
      objective way. Furthermore, in the field of software-assisted and navigated
      surgery medical image computing methods play a key role and have opened up new
      perspectives for patient treatment. However, further developments are needed to
      increase the grade of automation, accuracy, reproducibility and robustness.
      Moreover, the systems developed have to be integrated into the clinical workflow.
      METHODS: For the development of advanced image computing systems methods of
      different scientific fields have to be adapted and used in combination. The
      principal methodologies in medical image computing are the following: image
      segmentation, image registration, image analysis for quantification and computer 
      assisted image interpretation, modeling and simulation as well as visualization
      and virtual reality. Especially, model-based image computing techniques open up
      new perspectives for prediction of organ changes and risk analysis of patients
      and will gain importance in diagnostic and therapy of the future. RESULTS: From a
      methodical point of view the authors identify the following future trends and
      perspectives in medical image computing: development of optimized
      application-specific systems and integration into the clinical workflow, enhanced
      computational models for image analysis and virtual reality training systems,
      integration of different image computing methods, further integration of
      multimodal image data and biosignals and advanced methods for 4D medical image
      computing. CONCLUSIONS: The development of image analysis systems for diagnostic 
      support or operation planning is a complex interdisciplinary process. Image
      computing methods enable new insights into the patient's image data and have the 
      future potential to improve medical diagnostics and patient treatment.
FAU - Handels, H
AU  - Handels H
AD  - Department of Medical Informatics, University Medical Center Hamburg-Eppendorf,
      Martinistrasse 52, 20246 Hamburg, Germany. h.handels@uke.uni-hamburg.de
FAU - Ehrhardt, J
AU  - Ehrhardt J
LA  - eng
PT  - Journal Article
PL  - Germany
TA  - Methods Inf Med
JT  - Methods of information in medicine
JID - 0210453
SB  - IM
MH  - Diagnosis, Computer-Assisted/*instrumentation/methods
MH  - Echocardiography, Four-Dimensional/instrumentation/methods
MH  - Humans
MH  - Image Processing, Computer-Assisted/*instrumentation/methods
MH  - Medical Informatics/*trends
MH  - Therapy, Computer-Assisted/*instrumentation/methods
EDAT- 2009/01/20 09:00
MHDA- 2009/03/06 09:00
CRDT- 2009/01/20 09:00
AID - 09010011 [pii]
PST - ppublish
SO  - Methods Inf Med. 2009;48(1):11-7.

PMID- 18626137
OWN - NLM
STAT- MEDLINE
DA  - 20080715
DCOM- 20080905
IS  - 0957-4271 (Print)
IS  - 0957-4271 (Linking)
VI  - 17
IP  - 5-6
DP  - 2007
TI  - Effect of field of view on the levitation illusion.
PG  - 271-7
AB  - Supine subjects inside a furnished room in which both they and the room are
      pitched 90 degrees backwards may experience themselves and the room as upright
      relative to gravity. This effect is known as the levitation illusion because
      observers report that their arms feel weightless when extended, and objects
      hanging in the room seem to "levitate". This illusion is an extreme example of a 
      visually induced illusion of static tilt. Visually induced tilt illusions are
      commonly experienced in wide-screen movie theatres, flight simulators, and
      immersive virtual reality systems. For technical reasons an observer's field of
      view is often constrained in these environments. No studies have documented the
      effect of field-of-view (FOV) restriction on the incidence of the levitation
      illusion. Preliminary findings suggest that when concurrently manipulating the
      FOV and observer position within an environment, the incidence of levitation
      illusions depends not only on the field of view but also on the visible scene
      content.
FAU - Jenkin, H L
AU  - Jenkin HL
AD  - Department of Psychology, York University, Toronto, Canada. hjenkin@yorku.ca
FAU - Zacher, J E
AU  - Zacher JE
FAU - Jenkin, M R
AU  - Jenkin MR
FAU - Oman, C M
AU  - Oman CM
FAU - Harris, L R
AU  - Harris LR
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - Netherlands
TA  - J Vestib Res
JT  - Journal of vestibular research : equilibrium & orientation
JID - 9104163
SB  - IM
SB  - S
MH  - Adolescent
MH  - Adult
MH  - Female
MH  - Gravitation
MH  - Humans
MH  - *Illusions
MH  - Male
MH  - Middle Aged
MH  - *Orientation
MH  - Supine Position
MH  - Visual Perception/*physiology
EDAT- 2008/07/16 09:00
MHDA- 2008/09/06 09:00
CRDT- 2008/07/16 09:00
PST - ppublish
SO  - J Vestib Res. 2007;17(5-6):271-7.

PMID- 18238210
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20080201
DCOM- 20121002
IS  - 1083-4419 (Print)
IS  - 1083-4419 (Linking)
VI  - 33
IP  - 4
DP  - 2003
TI  - Superresolution modeling using an omnidirectional image sensor.
PG  - 607-15
LID - 10.1109/TSMCB.2003.814285 [doi]
AB  - Recently, many virtual reality and robotics applications have been called on to
      create virtual environments from real scenes. A catadioptric omnidirectional
      image sensor composed of a convex mirror can simultaneously observe a 360-degree 
      field of view making it useful for modeling man-made environments such as rooms, 
      corridors, and buildings, because any landmarks around the sensor can be taken in
      and tracked in its large field of view. However, the angular resolution of the
      omnidirectional image is low because of the large field of view captured. Hence, 
      the resolution of surface texture patterns on the three-dimensional (3-D) scene
      model generated is not sufficient for monitoring details. To overcome this, we
      propose a high resolution scene texture generation method that combines an
      omnidirectional image sequence using image mosaic and superresolution techniques.
FAU - Nagahara, H
AU  - Nagahara H
AD  - Graduate Sch. of Eng. Sci., Osaka Univ., Japan.
FAU - Yagi, Y
AU  - Yagi Y
FAU - Yachida, M
AU  - Yachida M
LA  - eng
PT  - Journal Article
PL  - United States
TA  - IEEE Trans Syst Man Cybern B Cybern
JT  - IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a
      publication of the IEEE Systems, Man, and Cybernetics Society
JID - 9890044
EDAT- 2008/02/02 09:00
MHDA- 2008/02/02 09:01
CRDT- 2008/02/02 09:00
AID - 10.1109/TSMCB.2003.814285 [doi]
PST - ppublish
SO  - IEEE Trans Syst Man Cybern B Cybern. 2003;33(4):607-15. doi:
      10.1109/TSMCB.2003.814285.

PMID- 18051092
OWN - NLM
STAT- MEDLINE
DA  - 20071204
DCOM- 20080103
LR  - 20091211
VI  - 10
IP  - Pt 1
DP  - 2007
TI  - Evaluation of a novel calibration technique for optically tracked oblique
      laparoscopes.
PG  - 467-74
AB  - This paper proposes an evaluation of a novel calibration method for an optically 
      tracked oblique laparoscope. We present the necessary tools to track an oblique
      scope and a camera model which includes changes to the intrinsic camera
      parameters thereby extending previously proposed methods. Because oblique scopes 
      offer a wide 'virtual' view on the surgical field, the method is of great
      interest for augmented reality guidance of laparoscopic interventions using an
      oblique scope. The model and an approximated version are evaluated in an
      extensive validation study. Using 5 sets of 40 calibration images, we compare
      both camera models (i.e. model and approximation) and 2 interpolation schemes.
      The selected model and interpolation scheme reaches an average accuracy of 2.60
      pixel and an equivalent 3D error of 0.60 mm. Finally, we present initial
      experience of the presented approach with an oblique scope and optical tracking
      in a clinical setup. During a laparoscopic rectum resection surgery the setup was
      used to augment the scene with a model of the pelvis. The method worked properly 
      and the attached probes did not interfere with normal procedure.
FAU - De Buck, Stijn
AU  - De Buck S
AD  - Faculty of Medicine, Medical Image Computing (ESAT and Radiology), K. U. Leuven, 
      Herestraat 49, 3000 Leuven, Belgium. stijn.debuck@uz.kuleuven.ac.be
FAU - Maes, Frederik
AU  - Maes F
FAU - D'Hoore, Andre
AU  - D'Hoore A
FAU - Suetens, Paul
AU  - Suetens P
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - Germany
TA  - Med Image Comput Comput Assist Interv
JT  - Medical image computing and computer-assisted intervention : MICCAI ...
      International Conference on Medical Image Computing and Computer-Assisted
      Intervention
JID - 101249582
SB  - IM
MH  - Calibration
MH  - Equipment Design
MH  - Equipment Failure Analysis/*methods/*standards
MH  - Image Interpretation, Computer-Assisted/*methods
MH  - Laparoscopes/*standards
MH  - Optics and Photonics/*instrumentation
MH  - Reference Values
MH  - Reproducibility of Results
MH  - Sensitivity and Specificity
MH  - Surgery, Computer-Assisted/*instrumentation/standards
EDAT- 2007/12/07 09:00
MHDA- 2008/01/04 09:00
CRDT- 2007/12/07 09:00
PST - ppublish
SO  - Med Image Comput Comput Assist Interv. 2007;10(Pt 1):467-74.

PMID- 17620142
OWN - NLM
STAT- MEDLINE
DA  - 20070816
DCOM- 20070821
LR  - 20140904
IS  - 1743-0003 (Electronic)
IS  - 1743-0003 (Linking)
VI  - 4
DP  - 2007
TI  - Pairing virtual reality with dynamic posturography serves to differentiate
      between patients experiencing visual vertigo.
PG  - 24
AB  - BACKGROUND: To determine if increased visual dependence can be quantified through
      its impact on automatic postural responses, we have measured the combined effect 
      on the latencies and magnitudes of postural response kinematics of transient
      optic flow in the pitch plane with platform rotations and translations. METHODS: 
      Six healthy (29-31 yrs) and 4 visually sensitive (27-57 yrs) subjects stood on a 
      platform rotated (6 deg of dorsiflexion at 30 deg/sec) or translated (5 cm at 5
      deg/sec) for 200 msec. Subjects either had eyes closed or viewed an immersive,
      stereo, wide field of view virtual environment (scene) moved in upward pitch for 
      a 200 msec period for three 30 sec trials at 5 velocities. RMS values and peak
      velocities of head, trunk, and head with respect to trunk were calculated. EMG
      responses of 6 trunk and lower limb muscles were collected and latencies and
      magnitudes of responses determined. RESULTS: No effect of visual velocity was
      observed in EMG response latencies and magnitudes. Healthy subjects exhibited
      significant effects (p < 0.05) of visual field velocity on peak angular
      velocities of the head. Head and trunk velocities and RMS values of visually
      sensitive subjects were significantly larger than healthy subjects (p < 0.05),
      but their responses were not modulated by visual field velocity. When examined
      individually, patients with no history of vestibular disorder demonstrated
      exceedingly large head velocities; patients with a history of vestibular disorder
      exhibited head velocities that fell within the bandwidth of healthy subjects.
      CONCLUSION: Differentiation of postural kinematics in visually sensitive subjects
      when exposed to the combined perturbations suggests that virtual reality
      technology could be useful for differential diagnosis and specifically designed
      interventions for individuals whose chief complaint is sensitivity to visual
      motion.
FAU - Keshner, Emily A
AU  - Keshner EA
AD  - Sensory Motor Performance Program, Rehabilitation Institute of Chicago, Room
      1406, 345 East Superior St, Chicago, IL 60611, USA. ekeshner@temple.edu
FAU - Streepey, Jefferson
AU  - Streepey J
FAU - Dhaher, Yasin
AU  - Dhaher Y
FAU - Hain, Timothy
AU  - Hain T
LA  - eng
PT  - Journal Article
DEP - 20070709
PL  - England
TA  - J Neuroeng Rehabil
JT  - Journal of neuroengineering and rehabilitation
JID - 101232233
SB  - IM
MH  - Adult
MH  - Biomechanical Phenomena
MH  - Case-Control Studies
MH  - Electromyography
MH  - Humans
MH  - Middle Aged
MH  - Postural Balance
MH  - Reaction Time
MH  - Vertigo/diagnosis/*physiopathology
MH  - *Visual Perception
PMC - PMC1948002
OID - NLM: PMC1948002
EDAT- 2007/07/11 09:00
MHDA- 2007/07/11 09:01
CRDT- 2007/07/11 09:00
PHST- 2007/01/12 [received]
PHST- 2007/07/09 [accepted]
PHST- 2007/07/09 [aheadofprint]
AID - 1743-0003-4-24 [pii]
AID - 10.1186/1743-0003-4-24 [doi]
PST - epublish
SO  - J Neuroeng Rehabil. 2007 Jul 9;4:24.

PMID- 17356211
OWN - NLM
STAT- MEDLINE
DA  - 20070314
DCOM- 20070503
IS  - 1077-2626 (Print)
IS  - 1077-2626 (Linking)
VI  - 13
IP  - 3
DP  - 2007 May-Jun
TI  - Egocentric depth judgments in optical, see-through augmented reality.
PG  - 429-42
AB  - Abstract-A fundamental problem in optical, see-through augmented reality (AR) is 
      characterizing how it affects the perception of spatial layout and depth. This
      problem is important because AR system developers need to both place graphics in 
      arbitrary spatial relationships with real-world objects, and to know that users
      will perceive them in the same relationships. Furthermore, AR makes possible
      enhanced perceptual techniques that have no real-world equivalent, such as x-ray 
      vision, where AR users are supposed to perceive graphics as being located behind 
      opaque surfaces. This paper reviews and discusses protocols for measuring
      egocentric depth judgments in both virtual and augmented environments, and
      discusses the well-known problem of depth underestimation in virtual
      environments. It then describes two experiments that measured egocentric depth
      judgments in AR. Experiment I used a perceptual matching protocol to measure AR
      depth judgments at medium and far-field distances of 5 to 45 meters. The
      experiment studied the effects of upper versus lower visual field location, the
      x-ray vision condition, and practice on the task. The experimental findings
      include evidence for a switch in bias, from underestimating to overestimating the
      distance of AR-presented graphics, at approximately 23 meters, as well as a
      quantification of how much more difficult the x-ray vision condition makes the
      task. Experiment II used blind walking and verbal report protocols to measure AR 
      depth judgments at distances of 3 to 7 meters. The experiment examined real-world
      objects, real-world objects seen through the AR display, virtual objects, and
      combined real and virtual objects. The results give evidence that the egocentric 
      depth of AR objects is underestimated at these distances, but to a lesser degree 
      than has previously been found for most virtual reality environments. The results
      are consistent with previous studies that have implicated a restricted
      field-of-view, combined with an inability for observers to scan the ground plane 
      in a near-to-far direction, as explanations for the observed depth
      underestimation.
FAU - Swan, J Edward 2nd
AU  - Swan JE 2nd
AD  - Department of Computer Science and Engineering, Institute for Neurocognitive
      Science and Technology, Mississippi State University, Butler Hall 39762, USA.
      swan@acm.org
FAU - Jones, Adam
AU  - Jones A
FAU - Kolstad, Eric
AU  - Kolstad E
FAU - Livingston, Mark A
AU  - Livingston MA
FAU - Smallman, Harvey S
AU  - Smallman HS
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
MH  - Computer Graphics
MH  - Depth Perception/*physiology
MH  - Humans
MH  - *User-Computer Interface
EDAT- 2007/03/16 09:00
MHDA- 2007/05/04 09:00
CRDT- 2007/03/16 09:00
AID - 10.1109/TVCG.2007.1035 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2007 May-Jun;13(3):429-42.

PMID- 17271394
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20070202
DCOM- 20070612
LR  - 20140821
IS  - 1557-170X (Print)
IS  - 1557-170X (Linking)
VI  - 7
DP  - 2004
TI  - Vestibular rehabilitation using a wide field of view virtual environment.
PG  - 4836-9
AB  - This paper presents a theoretical justification for using a wide field of view
      (FOV) virtual reality display system for use in vestibular rehabilitation. A wide
      FOV environment offers some unique features that may be beneficial to vestibular 
      rehabilitation. Primarily, optic flow information extracted from the periphery
      may be critical for recalibrating the sensory processes used by people with
      vestibular disorders. If this hypothesis is correct, then wide FOV systems will
      have an advantage over narrow field of view input devices such as head mounted or
      desktop displays. Devices that we have incorporated into our system that are
      critical for monitoring improvement in this clinical population will also be
      described.
FAU - Sparto, P J
AU  - Sparto PJ
AD  - Department of Physical Therapy, Pittsburgh University, PA, USA.
FAU - Furman, J M
AU  - Furman JM
FAU - Whitney, S L
AU  - Whitney SL
FAU - Hodges, L F
AU  - Hodges LF
FAU - Redfern, M S
AU  - Redfern MS
LA  - eng
PT  - Journal Article
PL  - United States
TA  - Conf Proc IEEE Eng Med Biol Soc
JT  - Conference proceedings : ... Annual International Conference of the IEEE
      Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and
      Biology Society. Annual Conference
JID - 101243413
EDAT- 2007/02/03 09:00
MHDA- 2007/02/03 09:01
CRDT- 2007/02/03 09:00
AID - 10.1109/IEMBS.2004.1404338 [doi]
PST - ppublish
SO  - Conf Proc IEEE Eng Med Biol Soc. 2004;7:4836-9.

PMID- 17218739
OWN - NLM
STAT- MEDLINE
DA  - 20070112
DCOM- 20070327
IS  - 1077-2626 (Print)
IS  - 1077-2626 (Linking)
VI  - 13
IP  - 2
DP  - 2007 Mar-Apr
TI  - Evaluation of a low-cost 3D sound system for immersive virtual reality training
      systems.
PG  - 204-12
AB  - Since Head Mounted Displays (HMD), datagloves, tracking systems, and powerful
      computer graphics resources are nowadays in an affordable price range, the usage 
      of PC-based "Virtual Training Systems" becomes very attractive. However, due to
      the limited field of view of HMD devices, additional modalities have to be
      provided to benefit from 3D environments. A 3D sound simulation can improve the
      capabilities of VR systems dramatically. Unfortunately, realistic 3D sound
      simulations are expensive and demand a tremendous amount of computational power
      to calculate reverberation, occlusion, and obstruction effects. To use 3D sound
      in a PC-based training system as a way to direct and guide trainees to observe
      specific events in 3D space, a cheaper alternative has to be provided, so that a 
      broader range of applications can take advantage of this modality. To address
      this issue, we focus in this paper on the evaluation of a low-cost 3D sound
      simulation that is capable of providing traceable 3D sound events. We describe
      our experimental system setup using conventional stereo headsets in combination
      with a tracked HMD device and present our results with regard to precision,
      speed, and used signal types for localizing simulated sound events in a virtual
      training environment.
FAU - Doerr, Kai-Uwe
AU  - Doerr KU
AD  - California Institute for Telecommunications and Information Technology, Irvine
      92697-2800, USA. kdorr@uci.edu
FAU - Rademacher, Holger
AU  - Rademacher H
FAU - Huesgen, Silke
AU  - Huesgen S
FAU - Kubbat, Wolfgang
AU  - Kubbat W
LA  - eng
PT  - Evaluation Studies
PT  - Journal Article
PL  - United States
TA  - IEEE Trans Vis Comput Graph
JT  - IEEE transactions on visualization and computer graphics
JID - 9891704
SB  - IM
MH  - Acoustics/*instrumentation
MH  - *Audiovisual Aids
MH  - Computer-Assisted Instruction/economics/*instrumentation/methods
MH  - *Ecosystem
MH  - Equipment Design
MH  - Equipment Failure Analysis
MH  - *Software
MH  - Sound
MH  - *Sound Localization
MH  - *User-Computer Interface
EDAT- 2007/01/16 09:00
MHDA- 2007/03/28 09:00
CRDT- 2007/01/16 09:00
AID - 10.1109/TVCG.2007.37 [doi]
PST - ppublish
SO  - IEEE Trans Vis Comput Graph. 2007 Mar-Apr;13(2):204-12.

PMID- 17074626
OWN - NLM
STAT- MEDLINE
DA  - 20061031
DCOM- 20070104
LR  - 20130520
IS  - 1526-5900 (Print)
IS  - 1526-5900 (Linking)
VI  - 7
IP  - 11
DP  - 2006 Nov
TI  - Virtual reality helmet display quality influences the magnitude of virtual
      reality analgesia.
PG  - 843-50
AB  - Immersive Virtual Reality (VR) distraction can be used in addition to traditional
      opioids to reduce procedural pain. The current study explored whether a
      High-Tech-VR helmet (ie, a 60-degree field-of-view head-mounted display) reduces 
      pain more effectively than a Low-Tech-VR helmet (a 35-degree field-of-view
      head-mounted display). Using a double-blind between-groups design, 77 healthy
      volunteers (no patients) aged 18-23 were randomly assigned to 1 of 3 groups. Each
      subject received a brief baseline thermal pain stimulus, and the same stimulus
      again minutes later while in SnowWorld using a Low-Tech-VR helmet (Group 1),
      using a High-Tech-VR helmet (Group 2), or receiving no distraction (Group 3,
      control group). Each participant provided subjective 0-10 ratings of cognitive,
      sensory, and affective components of pain, and amount of fun during the pain
      stimulus. Compared to the Low-Tech-VR helmet group, subjects in the High-Tech-VR 
      helmet group reported 34% more reduction in worst pain (P < .05), 46% more
      reduction in pain unpleasantness (P = .001), 29% more reduction in "time spent
      thinking about pain" (P < .05), and 32% more fun during the pain stimulus in VR
      (P < .05). Only 29% of participants in the Low-Tech helmet group, as opposed to
      65% of participants in the High-Tech-VR helmet group, showed a clinically
      significant reduction in pain intensity during virtual reality. These results
      highlight the importance of using an appropriately designed VR helmet to achieve 
      effective VR analgesia (see ). PERSPECTIVE: Pain during medical procedures (eg,
      burn wound care) is often excessive. Adjunctive virtual reality distraction can
      substantially reduce procedural pain. The results of the present study show that 
      a higher quality VR helmet was more effective at reducing pain than a lower
      quality VR helmet.
FAU - Hoffman, Hunter G
AU  - Hoffman HG
AD  - Human Interface Technology Laboratory, University of Washington, Seattle,
      Washington, USA. hunter@hitL.washington.edu
FAU - Seibel, Eric J
AU  - Seibel EJ
FAU - Richards, Todd L
AU  - Richards TL
FAU - Furness, Thomas A
AU  - Furness TA
FAU - Patterson, David R
AU  - Patterson DR
FAU - Sharar, Sam R
AU  - Sharar SR
LA  - eng
GR  - GM42725/GM/NIGMS NIH HHS/United States
GR  - HD37683/HD/NICHD NIH HHS/United States
PT  - Journal Article
PT  - Randomized Controlled Trial
PT  - Research Support, N.I.H., Extramural
PT  - Research Support, Non-U.S. Gov't
PL  - United States
TA  - J Pain
JT  - The journal of pain : official journal of the American Pain Society
JID - 100898657
SB  - IM
MH  - Adolescent
MH  - Adult
MH  - *Computer Graphics
MH  - Double-Blind Method
MH  - Female
MH  - Hot Temperature
MH  - Humans
MH  - Male
MH  - Pain/*prevention & control/*psychology
MH  - Pain Measurement
EDAT- 2006/11/01 09:00
MHDA- 2007/01/05 09:00
CRDT- 2006/11/01 09:00
PHST- 2005/07/05 [received]
PHST- 2006/04/03 [revised]
PHST- 2006/04/04 [accepted]
AID - S1526-5900(06)00768-1 [pii]
AID - 10.1016/j.jpain.2006.04.006 [doi]
PST - ppublish
SO  - J Pain. 2006 Nov;7(11):843-50.

PMID- 15818008
OWN - NLM
STAT- PubMed-not-MEDLINE
DA  - 20050408
DCOM- 20050517
LR  - 20061115
IS  - 0389-5386 (Print)
IS  - 0389-5386 (Linking)
VI  - 48
IP  - 5
DP  - 2004 Dec
TI  - [An evaluation of virtual waxing up system with haptic interface].
PG  - 751-60
AB  - PURPOSE: The purpose of this experiment was to construct a system of waxing
      pattern using virtual reality in the odontological field. The success of
      operation with feedback on the maniphalanx sense of waxing is reported. METHODS: 
      The constructed device was a system based on PHANToM DESKTOP (SensAble
      Technologies Inc.). This device includes six-degrees-of-freedom input and
      reproduction of the sense of touch by anti-power feedback. The software which
      controls this device enables four operations such as "Cutting down", "Piling up",
      "Melting", and "Finishing" to be applied to the virtual waxing pattern by virtual
      reality. Finally, the virtual waxing was examined by applying this system to a
      virtual abutment tooth. RESULTS: With six-degrees-of-freedom input and anti-power
      feedback, a virtual waxing and the designer's highly developed designs were
      successfully expressed regardless of the virtual space. CONCLUSIONS: 1. The
      virtual waxing up could be done by six-degrees-of-freedom input with the stylus
      in a virtual space. 2. The time taken for all these numerical values of
      corresponding rate of volume, occlusal, mesio-distal and bucco-lingual views to
      reach 100+/-5%ranged from 50 to 75 minutes, with an average of 63 minutes. 3. The
      virtual wax improvement showed the tendency to trace the passage of first
      arranging the volume, and straightening the outlook on the occlusal view at the
      end. 4. This system, which provides an interface between man machines,
      six-degrees-of-freedom input and anti-power feedback, has much in common with the
      existing waxing up and crafting. The demand for special technologies by technical
      persons and engineers can be kept to a minimum and application of the method can 
      be expanded into education and to objective evaluations.
FAU - Kikuchi, Motohiro
AU  - Kikuchi M
AD  - Department of Pediatric Dentistry, Nihon University School of Dentistry, Chiyoda,
      101-8310 Tokyo. kikuchi-m@dent.nihon-u.ac.jp
FAU - Kudo, Mariko
AU  - Kudo M
FAU - Seki, Nobuyuki
AU  - Seki N
FAU - Ihara, Masahiro
AU  - Ihara M
LA  - jpn
PT  - English Abstract
PT  - Journal Article
PL  - Japan
TA  - Nihon Hotetsu Shika Gakkai Zasshi
JT  - Nihon Hotetsu Shika Gakkai zasshi
JID - 7505724
EDAT- 2005/04/09 09:00
MHDA- 2005/04/09 09:01
CRDT- 2005/04/09 09:00
AID - 040579197 [pii]
PST - ppublish
SO  - Nihon Hotetsu Shika Gakkai Zasshi. 2004 Dec;48(5):751-60.

PMID- 15679946
OWN - NLM
STAT- Publisher
DA  - 20050131
IS  - 1743-0003 (Electronic)
IS  - 1743-0003 (Linking)
VI  - 1
IP  - 1
DP  - 2004 Dec 23
TI  - Simulator sickness when performing gaze shifts within a wide field of view optic 
      flow environment: preliminary evidence for using virtual reality in vestibular
      rehabilitation.
PG  - 14
AB  - BACKGROUND: Wide field of view virtual environments offer some unique features
      that may be beneficial for use in vestibular rehabilitation. For one, optic flow 
      information extracted from the periphery may be critical for recalibrating the
      sensory processes used by people with vestibular disorders. However, wide FOV
      devices also have been found to result in greater simulator sickness. Before a
      wide FOV device can be used in a clinical setting, its safety must be
      demonstrated. METHODS: Symptoms of simulator sickness were recorded by 9 healthy 
      adult subjects after they performed gaze shifting tasks to locate targets
      superimposed on an optic flow background. Subjects performed 8 trials of gaze
      shifting on each of the six separate visits. RESULTS: The incidence of symptoms
      of simulator sickness while subjects performed gaze shifts in an optic flow
      environment was lower than the average reported incidence for flight simulators. 
      The incidence was greater during the first visit compared with subsequent visits.
      Furthermore, the incidence showed an increasing trend over the 8 trials.
      CONCLUSION: The performance of head unrestrained gaze shifts in a wide FOV optic 
      flow environment is tolerated well by healthy subjects. This finding provides
      rationale for testing these environments in people with vestibular disorders, and
      supports the concept of using wide FOV virtual reality for vestibular
      rehabilitation.
FAU - Sparto, Patrick J
AU  - Sparto PJ
AD  - Department of Physical Therapy, University of Pittsburgh, Pittsburgh, PA, USA.
      psparto@pitt.edu.
FAU - Whitney, Susan L
AU  - Whitney SL
FAU - Hodges, Larry F
AU  - Hodges LF
FAU - Furman, Joseph M
AU  - Furman JM
FAU - Redfern, Mark S
AU  - Redfern MS
LA  - ENG
PT  - JOURNAL ARTICLE
DEP - 20041223
TA  - J Neuroeng Rehabil
JT  - Journal of neuroengineering and rehabilitation
JID - 101232233
PMC - PMC546407
EDAT- 2005/02/01 09:00
MHDA- 2005/02/01 09:00
CRDT- 2005/02/01 09:00
PHST- 2004/11/29 [received]
PHST- 2004/12/23 [accepted]
PHST- 2004/12/23 [aheadofprint]
AID - 1743-0003-1-14 [pii]
AID - 10.1186/1743-0003-1-14 [doi]
PST - epublish
SO  - J Neuroeng Rehabil. 2004 Dec 23;1(1):14.

PMID- 15455910
OWN - NLM
STAT- MEDLINE
DA  - 20040930
DCOM- 20041028
LR  - 20071114
IS  - 0926-9630 (Print)
IS  - 0926-9630 (Linking)
VI  - 94
DP  - 2003
TI  - Development of a training tool for endotracheal intubation: distributed augmented
      reality.
PG  - 288-94
AB  - The authors introduce a tool referred to as the Ultimate Intubation Head (UIH) to
      train medical practitioners' hand-eye coordination in performing endotracheal
      intubation with the help of augmented reality methods. In this paper we describe 
      the integration of a deployable UIH and present methods for augmented reality
      registration of real and virtual anatomical models. The assessment of the 52
      degrees field of view optics of the custom-designed and built head-mounted
      display is less than 1.5 arc minutes in the amount of blur and astigmatism, the
      two limiting optical aberrations. Distortion is less than 2.5%. Preliminary
      results of the registration of a physical phantom mandible on its virtual
      counterpart yields less than 3mm rms. in registration. Finally we describe an
      approach to distributed visualization where a given training procedure may be
      visualized and shared at various remote locations. Basic assessments of delays
      within two scenarios of data distribution were conducted and reported.
FAU - Rolland, Jannick
AU  - Rolland J
AD  - ODALab at School of Optics/CREOL, University of Central Florida and Research
      Park, Orlando FL, USA.
FAU - Davis, Larry
AU  - Davis L
FAU - Hamza-Lup, Felix
AU  - Hamza-Lup F
FAU - Daly, Jason
AU  - Daly J
FAU - Ha, Yonggang
AU  - Ha Y
FAU - Martin, Glenn
AU  - Martin G
FAU - Norfleet, Jack
AU  - Norfleet J
FAU - Thumann, Richard
AU  - Thumann R
FAU - Imielinska, Celina
AU  - Imielinska C
LA  - eng
GR  - 1-R29-LM06322-01A1/LM/NLM NIH HHS/United States
GR  - NLM99-103/DJH/LM/NLM NIH HHS/United States
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, Non-P.H.S.
PT  - Research Support, U.S. Gov't, P.H.S.
PL  - Netherlands
TA  - Stud Health Technol Inform
JT  - Studies in health technology and informatics
JID - 9214582
SB  - T
MH  - *Computer Simulation
MH  - Health Personnel/*education
MH  - *Intubation, Intratracheal
MH  - Manikins
MH  - User-Computer Interface
EDAT- 2004/10/01 05:00
MHDA- 2004/10/29 09:00
CRDT- 2004/10/01 05:00
PST - ppublish
SO  - Stud Health Technol Inform. 2003;94:288-94.

PMID- 15183704
OWN - NLM
STAT- MEDLINE
DA  - 20040608
DCOM- 20041021
LR  - 20041117
IS  - 0020-1383 (Print)
IS  - 0020-1383 (Linking)
VI  - 35 Suppl 1
DP  - 2004 Jun
TI  - Computer aided long bone fracture treatment.
PG  - S-A57-64
AB  - Intraoperative fluoroscopy is the tool for intraoperative control of long bone
      fracture reduction and osteosynthesis. Limitations of this technology include:
      High radiation exposure to the patient and the surgical team, limited field of
      view, image distortion, limitation to 2-D representations, and cumbersome
      updating of verification images. Fluoroscopy based navigation systems partially
      address these limitations by allowing fluoroscopic images to be used for
      real-time surgical localization and instrument tracking. In a clinical study on
      computer guidance by virtual fluoroscopy for distal locking, the capability to
      provide online guidance with significantly reduced fluoroscopy times is
      demonstrated. Virtual fluoroscopy applied for guidewire placement in a laboratory
      setup demonstrated the potential of the method to reduce procedure times, and the
      potential to increase precision of implant placement with decreased fluoroscopy
      times. By using virtual reality enhancement, starting from multiple registered
      fluoroscopy images, a virtual 3-D cylinder model for each principal bone fragment
      is reconstructed. This spatial cylinder model is not only used to supply a 3-D
      image of the fracture, but also allows effective fragment projection extraction
      from the fluoroscopic images and further achieves radiation-free updates of
      in-situ surgical fluoroscopic images through a non-linear interpolation and
      warping algorithm. After primary image acquisition, the image intensifier was
      replaced by the virtual reality system. It was shown that all the steps of the
      procedure, including fracture reduction and LISS osteosynthesis can be performed 
      completely in virtual reality.
FAU - Grutzner, Paul Alfred
AU  - Grutzner PA
AD  - BG Trauma Center Ludwigshafen, University of Heidelberg, 67071 Ludwigshafen,
      Germany. pa.gruetzner@urz.uni-heidelberg.de
FAU - Suhm, Norbert
AU  - Suhm N
LA  - eng
PT  - Clinical Trial
PT  - Controlled Clinical Trial
PT  - Journal Article
PL  - England
TA  - Injury
JT  - Injury
JID - 0226040
SB  - IM
MH  - Fluoroscopy
MH  - Fracture Fixation, Intramedullary/*methods
MH  - Hip Fractures/radiography/surgery
MH  - Humans
MH  - Image Processing, Computer-Assisted/methods
MH  - Imaging, Three-Dimensional
MH  - Intraoperative Care/methods
MH  - Leg Injuries/*surgery
MH  - Pilot Projects
MH  - Surgery, Computer-Assisted/*methods
EDAT- 2004/06/09 05:00
MHDA- 2004/10/22 09:00
CRDT- 2004/06/09 05:00
AID - 10.1016/j.injury.2004.05.011 [doi]
AID - S002013830400155X [pii]
PST - ppublish
SO  - Injury. 2004 Jun;35 Suppl 1:S-A57-64.

PMID- 15070110
OWN - NLM
STAT- MEDLINE
DA  - 20040408
DCOM- 20040421
LR  - 20051116
IS  - 0022-3085 (Print)
IS  - 0022-3085 (Linking)
VI  - 100
IP  - 4
DP  - 2004 Apr
TI  - Intraoperative stereoscopic QuickTime Virtual Reality.
PG  - 591-6
AB  - OBJECT: The aim of this study was to acquire intraoperative images during
      neurosurgical procedures for later reconstruction into a stereoscopic image
      system (QuickTime Virtual Reality [QTVR]) that would improve visualization of
      complex neurosurgical procedures. METHODS: A robotic microscope and digital
      cameras were used to acquire left and right image pairs during cranial surgery; a
      grid system facilitated image acquisition with the microscope. The surgeon
      determined a field of interest and a target or pivot point for image acquisition.
      Images were processed with commercially available software and hardware.
      Two-dimensional (2D) or interlaced left and right 2D images were reconstructed
      into a standard or stereoscopic QTVR format. Standard QTVR images were produced
      if stereoscopy was not needed. Intraoperative image sequences of regions of
      interest were captured in six patients. Relatively wide and deep dissections
      afford an opportunity for excellent QTVR production. Narrow or restricted
      surgical corridors can be reconstructed into the stereoscopic QTVR mode by using 
      a keyhole mode of image acquisition. The stereoscopic effect is unimpressive with
      shallow or cortical surface dissections, which can be reconstructed into standard
      QTVR images. CONCLUSIONS: The QTVR system depicts multiple views of the same
      anatomy from different angles. By tilting, panning, or rotating the reconstructed
      images, the user can view a virtual three-dimensional tour of a neurosurgical
      dissection, with images acquired intraoperatively. The stereoscopic QTVR format
      provides depth to the montage. The system recreates the dissection environment
      almost completely and provides a superior anatomical frame of reference compared 
      with the images captured by still or video photography in the operating room.
FAU - Balogh, Attila
AU  - Balogh A
AD  - Division of Neurological Surgery, Barrow Neurological Institute, St. Joseph's
      Hospital and Medical Center, Phoenix, Arizona 85013, USA.
FAU - Preul, Mark C
AU  - Preul MC
FAU - Schornak, Mark
AU  - Schornak M
FAU - Hickman, Michael
AU  - Hickman M
FAU - Spetzler, Robert F
AU  - Spetzler RF
LA  - eng
PT  - Journal Article
PT  - Review
PL  - United States
TA  - J Neurosurg
JT  - Journal of neurosurgery
JID - 0253357
SB  - AIM
SB  - IM
CIN - J Neurosurg. 2004 Apr;100(4):583; discussion 583-4. PMID: 15070108
MH  - Brain/*anatomy & histology/radiography
MH  - Humans
MH  - Imaging, Three-Dimensional
MH  - Intraoperative Period
MH  - Microscopy
MH  - Neurosurgery/education
MH  - Neurosurgical Procedures/*methods
MH  - Photography
MH  - Robotics
MH  - Signal Processing, Computer-Assisted
MH  - Software
MH  - Stereotaxic Techniques
MH  - *User-Computer Interface
MH  - Video Recording
RF  - 19
EDAT- 2004/04/09 05:00
MHDA- 2004/04/22 05:00
CRDT- 2004/04/09 05:00
AID - 10.3171/jns.2004.100.4.0591 [doi]
PST - ppublish
SO  - J Neurosurg. 2004 Apr;100(4):591-6.

PMID- 14756930
OWN - NLM
STAT- MEDLINE
DA  - 20040203
DCOM- 20040506
LR  - 20081121
IS  - 1094-9313 (Print)
IS  - 1094-9313 (Linking)
VI  - 6
IP  - 6
DP  - 2003 Dec
TI  - A magnet-friendly virtual reality fiberoptic image delivery system.
PG  - 645-8
AB  - A custom display was built into the MR radiofrequency headcoil to project
      high-resolution, wide field-of-view stereographic images. Advanced stimulus
      presentation technologies such as the one described could potentially contribute 
      to a better understanding of the relation between what people are thinking or
      experiencing, and their associated patterns of brain activity (www.vrpain.com).
FAU - Hoffman, Hunter G
AU  - Hoffman HG
AD  - Human Interface Technology Laboratory, University of Washington, Seattle,
      Washington 98195, USA. hunter@hitL.washington.edu
FAU - Richards, Todd L
AU  - Richards TL
FAU - Magula, Jeff
AU  - Magula J
FAU - Seibel, Eric J
AU  - Seibel EJ
FAU - Hayes, Cecil
AU  - Hayes C
FAU - Mathis, Mark
AU  - Mathis M
FAU - Sharar, Sam R
AU  - Sharar SR
FAU - Maravilla, Kenneth
AU  - Maravilla K
LA  - eng
GR  - HD40954-01/HD/NICHD NIH HHS/United States
GR  - P-50-33812/PHS HHS/United States
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PT  - Research Support, U.S. Gov't, P.H.S.
PL  - United States
TA  - Cyberpsychol Behav
JT  - Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual
      reality on behavior and society
JID - 9804397
SB  - IM
MH  - Computer Graphics/*instrumentation
MH  - *Computer Simulation
MH  - Equipment Design
MH  - Fiber Optic Technology/*instrumentation
MH  - Humans
MH  - Illusions/*psychology
MH  - Magnetic Resonance Imaging/instrumentation/*methods
MH  - Photic Stimulation/*instrumentation
MH  - *User-Computer Interface
EDAT- 2004/02/06 05:00
MHDA- 2004/05/07 05:00
CRDT- 2004/02/06 05:00
AID - 10.1089/109493103322725423 [doi]
PST - ppublish
SO  - Cyberpsychol Behav. 2003 Dec;6(6):645-8.

PMID- 12089824
OWN - NLM
STAT- MEDLINE
DA  - 20020701
DCOM- 20021001
LR  - 20061115
IS  - 0009-4722 (Print)
IS  - 0009-4722 (Linking)
VI  - 73
IP  - 5
DP  - 2002 May
TI  - [Image fusion, virtual reality, robotics and navigation. Effects on surgical
      practice].
PG  - 422-7
AB  - In the new minimally invasive surgical era, virtual reality, robotics, and image 
      merging have become topics on their own, offering the potential to revolutionize 
      current surgical treatment and assessment. Improved patient care in the digital
      age seems to be the primary impetus for continued efforts in the field of
      telesurgery. The progress in endoscopic surgery with regard to telesurgery is
      manifested by digitization of the pre-, intra-, and postoperative interaction
      with the patients' surgical disease via computer system integration: so-called
      Computer Assisted Surgery (CAS). The preoperative assessment can be improved by
      3D organ reconstruction, as in virtual colonoscopy or cholangiography, and by
      planning and practicing surgery using virtual or simulated organs. When
      integrating all of the data recorded during this preoperative stage, an enhanced 
      reality can be made possible to improve intra-operative patient interactions. CAS
      allows for increased three-dimensional accuracy, improved precision and the
      reproducibility of procedures. The ability to store the actions of the surgeon as
      digitized information also allows for universal, rapid distribution: i.e., the
      surgeon's activity can be transmitted to the other side of the operating room or 
      to a remote site via high-speed communications links, as was recently
      demonstrated by our own team during the Lindbergh operation. Furthermore, the
      surgeon will be able to share his expertise and skill through teleconsultation
      and telemanipulation, bringing the patient closer to the expert surgical team
      through electronic means and opening the way to advanced and continuous surgical 
      learning. Finally, for postoperative interaction, virtual reality and simulation 
      can provide us with 4 dimensional images, time being the fourth dimension. This
      should allow physicians to have a better idea of the disease process in
      evolution, and treatment modifications based on this view can be anticipated. We 
      are presently determining the accuracy and efficacy of 4 dimensional imaging
      compared to conventional evaluations.
FAU - Maresceaux, J
AU  - Maresceaux J
AD  - Institut de Recherche sur les Cancers de l'Appareil Digestif (IRCAD)-European
      Institute of Telesurgery (EITS), Universitat Louis Pasteur, 67091 Strasbourg,
      Frankreich. Jacques.Marescaux@ircad.u-strasbg.fr
FAU - Soler, L
AU  - Soler L
FAU - Ceulemans, R
AU  - Ceulemans R
FAU - Garcia, A
AU  - Garcia A
FAU - Henri, M
AU  - Henri M
FAU - Dutson, E
AU  - Dutson E
LA  - ger
PT  - English Abstract
PT  - Journal Article
TT  - Bildfusion, virtuelle Realitat, Robotik und Navigation. Einfluss auf die
      chirurgische Praxis.
PL  - Germany
TA  - Chirurg
JT  - Der Chirurg; Zeitschrift fur alle Gebiete der operativen Medizen
JID - 16140410R
SB  - IM
MH  - Computer Systems/trends
MH  - Diagnostic Imaging/*instrumentation
MH  - Forecasting
MH  - Germany
MH  - Humans
MH  - Image Processing, Computer-Assisted/*instrumentation
MH  - Imaging, Three-Dimensional/*instrumentation
MH  - Surgery, Computer-Assisted/*instrumentation
MH  - Telemedicine/*instrumentation
MH  - *User-Computer Interface
EDAT- 2002/07/02 10:00
MHDA- 2002/10/03 04:00
CRDT- 2002/07/02 10:00
AID - 10.1007/s00104-002-0473-x [doi]
PST - ppublish
SO  - Chirurg. 2002 May;73(5):422-7.

PMID- 12044746
OWN - NLM
STAT- MEDLINE
DA  - 20020604
DCOM- 20020930
LR  - 20061115
IS  - 0042-6989 (Print)
IS  - 0042-6989 (Linking)
VI  - 42
IP  - 11
DP  - 2002 May
TI  - Visual perception of planar orientation: dominance of static depth cues over
      motion cues.
PG  - 1403-12
AB  - We measured the ability to report the tilt (direction of maximal slope) of a
      plane under monocular viewing conditions, from static depth cues (square grid
      patterns) and motion parallax (small rotations of the plane about a
      frontoparallel axis). These two cues were presented separately, or
      simultaneously. In the latter case they specified tilts that were either
      collinear (coherent case) or orthogonal (conflict case). The field of view was
      small (8 degrees) or large (60 degrees). In small field, for motion parallax, the
      reported tilt depends strongly on the orientation of the plane relative to the
      rotation axis, being totally ambiguous when tilt is collinear with the rotation
      axis. In contrast, in large field, the reported tilt depends little on this
      variable, and is accurately specified by motion cues. In both cases static cues
      strongly dominated the tilt reports. Hence static grid patterns constitute robust
      tilt cues, which can dominate contradictory tilt indications from motion
      parallax, and should be considered as essential for the visual orientation during
      locomotion, or the immersion in virtual reality environments.
FAU - Cornilleau-Peres, Valerie
AU  - Cornilleau-Peres V
AD  - IPAL-CNRS, KRDL, 21 Heng Mui Keng Terrace, 119613, Singapore\.
      rjizaac@singnet.com.sg
FAU - Wexler, Mark
AU  - Wexler M
FAU - Droulez, Jacques
AU  - Droulez J
FAU - Marin, Emmanuel
AU  - Marin E
FAU - Miege, Christian
AU  - Miege C
FAU - Bourdoncle, Bernard
AU  - Bourdoncle B
LA  - eng
PT  - Journal Article
PT  - Research Support, Non-U.S. Gov't
PL  - England
TA  - Vision Res
JT  - Vision research
JID - 0417402
SB  - IM
MH  - Adult
MH  - *Cues
MH  - *Depth Perception
MH  - Female
MH  - Humans
MH  - Male
MH  - *Motion Perception
MH  - Orientation
MH  - Pattern Recognition, Visual
MH  - Photic Stimulation/methods
MH  - Rotation
MH  - Vision, Monocular
EDAT- 2002/06/05 10:00
MHDA- 2002/10/02 04:00
CRDT- 2002/06/05 10:00
AID - S004269890100298X [pii]
PST - ppublish
SO  - Vision Res. 2002 May;42(11):1403-12.

PMID- 10601525
OWN - NLM
STAT- MEDLINE
DA  - 20000210
DCOM- 20000210
LR  - 20081121
IS  - 1055-3207 (Print)
IS  - 1055-3207 (Linking)
VI  - 9
IP  - 1
DP  - 2000 Jan
TI  - Virtual reality in surgical training.
PG  - 61-79, vii
AB  - Virtual reality in surgery and, more specifically, in surgical training, faces a 
      number of challenges in the future. These challenges are building realistic
      models of the human body, creating interface tools to view, hear, touch, feel,
      and manipulate these human body models, and integrating virtual reality systems
      into medical education and treatment. A final system would encompass simulators
      specifically for surgery, performance machines, telemedicine, and telesurgery.
      Each of these areas will need significant improvement for virtual reality to
      impact medicine successfully in the next century. This article gives an overview 
      of, and the challenges faced by, current systems in the fast-changing field of
      virtual reality technology, and provides a set of specific milestones for a truly
      realistic virtual human body.
FAU - Lange, T
AU  - Lange T
AD  - Hannover Medical School, Hannover, Germany.
FAU - Indelicato, D J
AU  - Indelicato DJ
FAU - Rosen, J M
AU  - Rosen JM
LA  - eng
PT  - Journal Article
PT  - Review
PL  - UNITED STATES
TA  - Surg Oncol Clin N Am
JT  - Surgical oncology clinics of North America
JID - 9211789
SB  - IM
MH  - Computer Simulation
MH  - Computer-Assisted Instruction/instrumentation/*methods/trends
MH  - Education, Medical, Graduate/*methods/trends
MH  - Forecasting
MH  - General Surgery/*education
MH  - Humans
MH  - Models, Anatomic
MH  - Telemedicine/organization & administration
MH  - *User-Computer Interface
RF  - 51
EDAT- 1999/12/22
MHDA- 1999/12/22 00:01
CRDT- 1999/12/22 00:00
PST - ppublish
SO  - Surg Oncol Clin N Am. 2000 Jan;9(1):61-79, vii.

PMID- 9635304
OWN - NLM
STAT- MEDLINE
DA  - 19980811
DCOM- 19980811
LR  - 20061115
IS  - 0301-2603 (Print)
IS  - 0301-2603 (Linking)
VI  - 26
IP  - 6
DP  - 1998 Jun
TI  - [Discrepancy between surgeon's binocular parallax perception and manipulation in 
      the neurosurgical operation].
PG  - 517-22
AB  - The application of virtual reality (VR) to the neurosurgical field has been
      increasing recently, however, the relation between the surgeon and the VR
      environment is rarely studied. We examined the trajectory of a surgical
      instrument during manipulation of a virtual object using a video-see-through
      microscope and a neurosurgical navigator (CANS Navigator) to find better
      surgeon-microscope interface. A resin cylindrical phantom was produced
      representing the surgical field, which included two 3 dimensionally arranged
      small spheres and a virtual 'gate'. The phantom was fixed and set under the
      microscope with a skull clamp mimicking conditions in an ordinary craniotomy.
      Firstly, the binocular parallax perception under microscope was examined.
      Experienced and inexperienced neurosurgeons were asked to learn the position of
      the virtual 'gate' for 3 minutes. Then, after 5 minutes to point with the
      navigator probe (suction tube), under various conditions; under the naked eyes,
      under the microscope, under the navigator without observing the phantom, and
      under the microscope with picture in picture (PIP) display of the navigational
      image. The positions of the suction tube were recorded at real time into the
      navigator for later analysis. Secondly, the task performance in this VR
      environment was studied by analyzing the trajectory of the suction tube from one 
      sphere to the other sphere passing the virtual 'gate' under various conditions. A
      significant difference in pointing precision between experienced and
      inexperienced neurosurgeons was able to be observed only under microscope. This
      difference was mainly derived from overestimation of the depth of the virtual
      'gate' by the inexperienced neurosurgeons. Among the above conditions, pointing
      under the microscope with PIP was able to be performed the most precisely and the
      most promptly. This study disclosed the presence of stereoscopic distortion in
      the microscope. The PIP display of the navigational image in the microscopic view
      remarkably improved the task performance, which could be accounted for by the
      correction of the somewhat distorted binocular parallax perception under the
      neurosurgical microscope by the provision of another visual key.
FAU - Kato, A
AU  - Kato A
AD  - Department of Neurosurgery, Osaka University Medical School, Japan.
FAU - Hirata, M
AU  - Hirata M
FAU - Yoshimine, T
AU  - Yoshimine T
FAU - Tamura, S
AU  - Tamura S
FAU - Kishino, F
AU  - Kishino F
FAU - Hayakawa, T
AU  - Hayakawa T
LA  - jpn
PT  - English Abstract
PT  - Journal Article
PL  - JAPAN
TA  - No Shinkei Geka
JT  - No shinkei geka. Neurological surgery
JID - 0377015
SB  - IM
MH  - Brain/*surgery
MH  - *Computer Simulation
MH  - *Depth Perception
MH  - Humans
MH  - User-Computer Interface
EDAT- 1998/06/23
MHDA- 1998/06/23 00:01
CRDT- 1998/06/23 00:00
PST - ppublish
SO  - No Shinkei Geka. 1998 Jun;26(6):517-22.

PMID- 9509920
OWN - NLM
STAT- MEDLINE
DA  - 19980504
DCOM- 19980504
LR  - 20041117
IS  - 0897-3806 (Print)
IS  - 0897-3806 (Linking)
VI  - 11
IP  - 2
DP  - 1998
TI  - The virtual anatomy practical: a stereoscopic 3D interactive multimedia computer 
      examination program.
PG  - 89-94
AB  - Continuing advances in computer visualization and interface technologies have
      enabled development of "virtual reality" programs that allow users to perceive
      and to interact with objects in artificial three-dimensional environments. Such
      technologies were used to create an image database and program for administering 
      a practical examination in human gross anatomy. Stereoscopic image pairs of
      prepared laboratory dissections were digitized from multiple views of the thorax,
      abdomen, pelvic region, and upper and lower extremities. For each view, the
      stereo pairs were interlaced into a single, field-sequential stereoscopic picture
      using an image processing program. The resulting color-corrected, interlaced
      image files were organized in a database stored on a large-capacity hard disk.
      Selected views were provided with structural identification pointers and letters 
      (A and B). For each view, appropriate two-part examination questions were spoken 
      by a human narrator, digitally recorded, and saved as universal audio format
      files on the archival hard disk. Images and digital narration were organized in
      an interactive multimedia program created with a high-level multimedia authoring 
      system. At run-time, 24-bit color 3D images were displayed on a large-screen
      computer monitor and observed through liquid crystal shutter goggles. A 90-second
      interval timer and tone were provided to give student users a time limit for each
      question comparable to that of a conventional practical examination. Users could 
      control the program and select regional "subexams" using a mouse and cursor to
      point-and-click on screen-level control words ("buttons").
FAU - Trelease, R B
AU  - Trelease RB
AD  - Department of Neurobiology, UCLA School of Medicine, Los Angeles, California
      90095, USA. trelease@ucla.edu
LA  - eng
PT  - Journal Article
PL  - UNITED STATES
TA  - Clin Anat
JT  - Clinical anatomy (New York, N.Y.)
JID - 8809128
SB  - IM
MH  - *Anatomy, Regional
MH  - Computer-Assisted Instruction/methods
MH  - Education, Medical, Undergraduate/methods
MH  - Educational Measurement/*methods
MH  - Educational Technology
MH  - Humans
MH  - Multimedia
MH  - *User-Computer Interface
EDAT- 1998/03/24 03:01
MHDA- 2000/06/20 09:00
CRDT- 1998/03/24 03:01
AID - 10.1002/(SICI)1098-2353(1998)11:2<89::AID-CA4>3.0.CO;2-N [pii]
AID - 10.1002/(SICI)1098-2353(1998)11:2<89::AID-CA4>3.0.CO;2-N [doi]
PST - ppublish
SO  - Clin Anat. 1998;11(2):89-94.

PMID- 9276874
OWN - NLM
STAT- MEDLINE
DA  - 19971202
DCOM- 19971202
LR  - 20121115
IS  - 0301-4894 (Print)
IS  - 0301-4894 (Linking)
VI  - 98
IP  - 7
DP  - 1997 Jul
TI  - [Surgical treatment and research of esophageal cancer in the past and the present
      era in our department--for the view point of the future].
PG  - 649-54
AB  - We describe the result of esophageal cancer treatment in the past era of the
      Department of Surgery, Chiba University School of Medicine directed by Professor 
      Seo, Nakayama and Sato and the improved results of the treatment at the present
      time from the view point of diagnosis (early superficial cancer, lymph node
      metastasis and adjacent organ invasion) and treatment (three-field lymph node
      dissection, improved result of operative mortality and morbidity, improvement of 
      long-term survival rate). In addition, we prospect the future through the present
      study such as mutation of P53 gene and malignancy, immunotherapy using cytokine
      gene transfer, cancer inhibition therapy by induction of cancer suppressor gene, 
      prodrug therapy, heavy particle iron therapy, and clinical application of virtual
      reality to the surgical field.
FAU - Isono, K
AU  - Isono K
AD  - Department of Surgery, Chiba University School of Medicine, Japan.
LA  - jpn
PT  - English Abstract
PT  - Lectures
PL  - JAPAN
TA  - Nihon Geka Gakkai Zasshi
JT  - Nihon Geka Gakkai zasshi
JID - 0405405
SB  - IM
MH  - Esophageal Neoplasms/diagnosis/*surgery
MH  - Genes, p53
MH  - Genetic Therapy
MH  - Humans
MH  - Immunotherapy
MH  - Lymph Node Excision
MH  - Lymphatic Metastasis
MH  - Medical Oncology/*trends
MH  - Mutation
MH  - Tomography, Emission-Computed
EDAT- 1997/07/01
MHDA- 1997/07/01 00:01
CRDT- 1997/07/01 00:00
PST - ppublish
SO  - Nihon Geka Gakkai Zasshi. 1997 Jul;98(7):649-54.

PMID- 9228336
OWN - NLM
STAT- MEDLINE
DA  - 19970930
DCOM- 19970930
LR  - 20141120
IS  - 0946-7211 (Print)
IS  - 0946-7211 (Linking)
VI  - 40
IP  - 2
DP  - 1997 Jun
TI  - Enhancing neurosurgical endoscopy with the use of 'virtual reality' headgear.
PG  - 47-9
AB  - Widespread use of neurosurgical endoscopy has been hampered by the necessity of
      looking up from the surgical field to view the endoscopic image on a video
      monitor. Here is demonstrated a simple, lightweight headpiece with a small,
      built-in video monitor that reflects the video image to the dominant eye, thus
      allowing the operator to continuously observe the surgical field either via
      peripheral vision or via the non-dominant eye. Successful use of the device is
      documented here in a minimally invasive fenestration of an arachnoid cyst into
      the lateral ventricle via a flexible endoscopic procedure.
FAU - McGregor, J M
AU  - McGregor JM
AD  - Division of Neurosurgery, The Ohio State University, Columbus 43210, USA.
LA  - eng
PT  - Case Reports
PT  - Journal Article
PL  - GERMANY
TA  - Minim Invasive Neurosurg
JT  - Minimally invasive neurosurgery : MIN
JID - 9440973
SB  - IM
MH  - Adult
MH  - Arachnoid Cysts/*surgery
MH  - Computer Terminals
MH  - *Data Display
MH  - *Endoscopes
MH  - Endoscopy/methods
MH  - Female
MH  - Humans
MH  - Minimally Invasive Surgical Procedures/instrumentation/methods
MH  - Monitoring, Intraoperative/*instrumentation
MH  - *User-Computer Interface
EDAT- 1997/06/01
MHDA- 1997/06/01 00:01
CRDT- 1997/06/01 00:00
AID - 10.1055/s-2008-1053414 [doi]
PST - ppublish
SO  - Minim Invasive Neurosurg. 1997 Jun;40(2):47-9.

PMID- 10388041
OWN - NLM
STAT- Publisher
DA  - 19990630
IS  - 1549-490X (Electronic)
IS  - 1083-7159 (Linking)
VI  - 2
IP  - 2
DP  - 1997
TI  - On the Way to New Horizons: Telemedicine in Oncology.
PG  - III-IV
AB  - Breathtaking insights into carcinogenesis and tumor biology have been gained
      mainly by recent technical advances in molecular-biological and genetic
      techniques. Thus, dimensions of earlier diagnosis and the development of new
      concepts in therapy arise, which were previously unavailable. There is no doubt
      that through these techniques the future role and tasks of surgical oncology will
      change. New indications will result, for example, in the context of prophylactic 
      therapy of hereditary malignant disease or the removal of tissue predisposed to
      tumors. However, modes of therapy orientated toward molecular biology will still 
      be dependent on specialist surgical interventions in the future. Examples are
      such innovative concepts of therapy as transport of a therapeutic device to or
      into tumor cells (e.g., gene gun), or even simply obtaining the necessary tumor
      tissue for therapy (vaccination with transfected autologous tumor cells).
      Therefore, the future of surgical oncology will be influenced quantitatively as
      well as conceptually by new qualitative requirements. Improving precision of the 
      surgical intervention will have to go hand-in-hand with a further reduction in
      surgical trauma. The consistent use of laser, video, computer and communication
      technology can be seen as an important predeterminant here for optimizing
      diagnostic and therapeutic procedures. If correctly guided, the professional
      experience of the individual surgeon and his personal efficiency can also be
      positively influenced by the swift conversion of society to multimedia and
      information technology. Major advances in interdisciplinary communication, as one
      important factor in the choice and the course of suitable complex therapies in
      oncology, will have to target and help to overcome former weak spots.
      Communication in and outside one department or hospital, as well as external
      communication between different medical disciplines and specialists, is being
      developed further and increasingly refined. The possibilities of modern
      technology in addition to verbal exchange include visual and interactive
      "tele"-communication. This renders a new option to the physician, as without
      direct patient contact he is able to observe, counsel and actively interact - the
      latter even more so in the future. In oncology the increase of knowledge thus far
      has gone hand-in-hand with further specialization. This explains the difficulties
      one encounters in the correct evaluation of relevant data of one specific
      patient. Telemedicine will help to focus on the advantages of specialist
      knowledge by rendering access to all available data. These possibilities should
      furthermore be accessible during a consultation, an examination or in the course 
      of a surgical intervention. Real-time modalities are referred to as telepresence 
      and exceed by far a mere electronic version of the patient's medical folder.
      Especially in oncology, interdisciplinary collaboration is immensely important
      for successful therapy. Preoperative diagnostic data are still to be evaluated
      according to the intraoperative findings. At this decisive moment, it is
      necessary to involve specialists of other oncological disciplines. Real-time
      communication devices have to be present in order to transfer image data and
      clinical observations and ensure the best possible transmission quality to
      resident and geographically distant experts. With further technological
      perfection and widespread availability of interactive consultation, other
      applications include the "second opinion" in the daily routine. Another
      fascinating option in oncology is offered by visual computer simulation in
      virtual reality (VR). Medical data are visualized according to the human
      perception by the means of scenic simulation. From that point of view, VR
      technologies represent a practicable user interface between computer technology
      and the individual human being. Through VR, three-dimensional worlds containing
      virtual objects, which consist of computer-generated data, are created, which the
      user may explore and liberally interact with. The perfect simulation of realistic
      settings offers a method of training that may be extended to the field of
      oncology, as it has been known for a comparatively long period of time from
      flight simulators in space and air technology. In contrast, medical training is
      currently achieved mainly by "training-on-the-job." There is well-proven and
      widely acknowledged certainty of the tremendous influence that the number of
      surgical interventions-in other words, the training skills of the surgeon-has on 
      the success of a diagnostic or therapeutic intervention. Previously, the
      subjective experience of the physician acquired from earlier cases determined his
      efficiency to a large extent. It was, in addition, influenced essentially by
      perception, "performance on the day" and personal attitude. The goal must be to
      strengthen the objective criteria as the basis for consistent decision-making
      processes and clear instructions for therapy. Strict quality management as
      practiced in air technology has clearly led to a reduction in accidents, and,
      accordingly, a similar effect is imaginable in oncology with continuous training 
      using VR simulators, leading to improved therapeutic outcome. Other possibilities
      for use are principally implied and similarly useful for medical school and
      postgraduate training. The idea of computer-guided medical procedures or medical 
      robots is therefore no longer a mere utopia. Telepresence, telerobotic and VR
      techniques should, in principle, effectively support the physician in diagnostic 
      processes and therapy. The responsibility for coordination and sensible use of
      new technological developments will still remain with the physician, such as
      improving and simplifying medical procedures. Technology should be used according
      to the situation, not to adapt the patient to a technocratic environment, but to 
      emphasize human treatment of the individual patient. From the opinion of the
      telephone being a futile technical invention to the other extreme of computed
      technology as a substitute for the physician (Dr. Cyber), the future role of
      telemedical techniques and their potential for medical advantage or support,
      especially in the field of oncology, should be critically viewed and evaluated.
FAU - Schlag
AU  - Schlag PM
AD  - Medizinische Fakultat der Humboldt Universitat zu Berlin, Surgical Oncology,
      Robert-Rossle-Klinik, 13122 Berlin, Germany. schlag@rrk-berlin.de
LA  - ENG
PT  - JOURNAL ARTICLE
TA  - Oncologist
JT  - The oncologist
JID - 9607837
EDAT- 1997/01/01 00:00
MHDA- 1999/07/01 00:00
CRDT- 1997/01/01 00:00
PST - ppublish
SO  - Oncologist. 1997;2(2):III-IV.

PMID- 10184810
OWN - NLM
STAT- MEDLINE
DA  - 19980212
DCOM- 19980212
LR  - 20041117
IS  - 0926-9630 (Print)
IS  - 0926-9630 (Linking)
VI  - 44
DP  - 1997
TI  - Development of a virtual sand box: an application of virtual environment for
      psychological treatment.
PG  - 113-20
AB  - The sand play technique has often been used in psychological treatments or in the
      diagnosis of autism patients. In this paper, the prototype application called
      "virtual sand box" is developed as a virtual environment to support this
      technique. Experimental results show the advantages of applying virtual reality
      technology to clinical medicine; particularly with respect to the diagnosis of
      people with psychological and psychiatrical difficulties such as autism and
      neurosis. The actual system has been implemented by using a graphics workstation,
      a wide-view field display, and 3D input devices.
FAU - Hirose, M
AU  - Hirose M
AD  - Faculty of Engineering, University of Tokyo, Japan. hirose@ihl.t.u-tokyo.ac.jp
FAU - Kijima, R
AU  - Kijima R
FAU - Shirakawa, K
AU  - Shirakawa K
FAU - Nihei, K
AU  - Nihei K
LA  - eng
PT  - Journal Article
PL  - NETHERLANDS
TA  - Stud Health Technol Inform
JT  - Studies in health technology and informatics
JID - 9214582
SB  - T
MH  - Autistic Disorder/*therapy
MH  - Child
MH  - Computer Graphics/*instrumentation
MH  - *Computer Simulation
MH  - Humans
EDAT- 1996/12/08
MHDA- 1996/12/08 00:01
CRDT- 1996/12/08 00:00
PST - ppublish
SO  - Stud Health Technol Inform. 1997;44:113-20.

PMID- 8793223
OWN - NLM
STAT- MEDLINE
DA  - 19961114
DCOM- 19961114
LR  - 20041117
IS  - 0897-3806 (Print)
IS  - 0897-3806 (Linking)
VI  - 9
IP  - 4
DP  - 1996
TI  - Toward virtual anatomy: a stereoscopic 3-D interactive multimedia computer
      program for cranial osteology.
PG  - 269-72
AB  - Advances in computer visualization and user interface technologies have enabled
      development of "virtual reality" programs that allow users to perceive and to
      interact with objects in artificial three-dimensional environments. Such
      technologies were used to create an image database and program for studying the
      human skull, a specimen that has become increasingly expensive and scarce.
      Stereoscopic image pairs of a museum-quality skull were digitized from multiple
      views. For each view, the stereo pairs were interlaced into a single,
      field-sequential stereoscopic picture using an image processing program. The
      resulting interlaced image files are organized in an interactive multimedia
      program. At run-time, gray-scale 3-D images are displayed on a large-screen
      computer monitor and observed through liquid-crystal shutter goggles. Users can
      then control the program and change views with a mouse and cursor to
      point-and-click on screen-level control words ("buttons"). For each view of the
      skull, an ID control button can be used to overlay pointers and captions for
      important structures. Pointing and clicking on "hidden buttons" overlying certain
      structures triggers digitized audio spoken word descriptions or mini lectures.
FAU - Trelease, R B
AU  - Trelease RB
AD  - Department of Anatomy and Cell Biology, UCLA School of Medicine 90095, USA.
LA  - eng
PT  - Journal Article
PL  - UNITED STATES
TA  - Clin Anat
JT  - Clinical anatomy (New York, N.Y.)
JID - 8809128
SB  - IM
MH  - Adult
MH  - Anatomy, Cross-Sectional/*education
MH  - Computer-Assisted Instruction/*methods
MH  - Data Display
MH  - Education, Medical, Undergraduate/*methods
MH  - Humans
MH  - Image Interpretation, Computer-Assisted/methods
MH  - *Multimedia
MH  - Skull/*anatomy & histology
MH  - Software
MH  - Teaching Materials
MH  - *User-Computer Interface
EDAT- 1996/01/01
MHDA- 1996/01/01 00:01
CRDT- 1996/01/01 00:00
AID - 10.1002/(SICI)1098-2353(1996)9:4<269::AID-CA10>3.0.CO;2-P [pii]
AID - 10.1002/(SICI)1098-2353(1996)9:4<269::AID-CA10>3.0.CO;2-P [doi]
PST - ppublish
SO  - Clin Anat. 1996;9(4):269-72.
